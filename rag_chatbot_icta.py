#!/usr/bin/env python3
"""
RAG Chatbot ICTA Technology - Vers√£o Simplificada com Menus Interativos
Sistema amig√°vel para cria√ß√£o de chatbot FAQ

Autor: Jesse Fernandes - ICTA Technology
GitHub: https://github.com/jesseff20/rag-chatbot
"""

from __future__ import annotations
import os
import json
import sys
import time
from datetime import datetime
from dataclasses import dataclass
from typing import Any, List, Dict, Optional

# Imports principais
import faiss  # type: ignore
import numpy as np
from tqdm import tqdm
from colorama import Fore, Style, init, Back
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Inicializar colorama para Windows
init(autoreset=True)

# ================================
# Configura√ß√µes Padr√£o
# ================================

DEFAULT_CONFIG = {
    "docs_path": "./data",
    "index_path": "./index/faiss.index",
    "meta_path": "./index/meta.jsonl",
    "settings_path": "./index/settings.json",
    "history_path": "./history/chat_history.jsonl",
    "chunk_size": 800,
    "overlap": 120,
    "top_k": 3,
    "max_tokens": 150,
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    "generation_model": "google/flan-t5-base"
}

# ================================
# Classes de Dados
# ================================

@dataclass
class Metadata:
    source: str
    chunk_id: int
    start_char: int
    end_char: int

@dataclass
class Retrieved:
    text: str
    meta: Metadata
    score: float

# ================================
# Utilit√°rios de Interface
# ================================

def print_header():
    """Imprime cabe√ßalho do programa"""
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}ü§ñ RAG Chatbot ICTA Technology - Vers√£o Simplificada")
    print(f"{Fore.CYAN}{'='*60}")
    print(f"{Fore.GREEN}Sistema interativo para cria√ß√£o de chatbot FAQ")
    print(f"{Fore.GREEN}GitHub: https://github.com/jesseff20/rag-chatbot")
    print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}\n")

def print_menu_option(number: int, title: str, description: str):
    """Imprime uma op√ß√£o do menu formatada"""
    print(f"{Fore.YELLOW}{number:2}. {Fore.WHITE}{title}")
    print(f"    {Fore.LIGHTBLACK_EX}{description}{Style.RESET_ALL}")

def get_user_choice(max_option: int) -> int:
    """Obt√©m escolha do usu√°rio com valida√ß√£o"""
    while True:
        try:
            choice = input(f"\n{Fore.CYAN}üéØ Escolha uma op√ß√£o (1-{max_option}): {Style.RESET_ALL}")
            choice_num = int(choice)
            if 1 <= choice_num <= max_option:
                return choice_num
            else:
                print(f"{Fore.RED}‚ùå Por favor, escolha um n√∫mero entre 1 e {max_option}")
        except ValueError:
            print(f"{Fore.RED}‚ùå Por favor, digite um n√∫mero v√°lido")
        except KeyboardInterrupt:
            print(f"\n{Fore.YELLOW}üëã Saindo do programa...")
            sys.exit(0)

def confirm_action(message: str) -> bool:
    """Confirma uma a√ß√£o com o usu√°rio"""
    while True:
        response = input(f"{Fore.YELLOW}‚ùì {message} (s/n): {Style.RESET_ALL}").lower().strip()
        if response in ['s', 'sim', 'y', 'yes']:
            return True
        elif response in ['n', 'nao', 'n√£o', 'no']:
            return False
        else:
            print(f"{Fore.RED}‚ùå Digite 's' para sim ou 'n' para n√£o")

def wait_for_enter():
    """Espera o usu√°rio pressionar Enter"""
    _ = input(f"\n{Fore.LIGHTBLACK_EX}üìù Pressione Enter para continuar...{Style.RESET_ALL}")

# ================================
# Utilit√°rios de Processamento
# ================================

def read_text_files(folder: str) -> dict[str, str]:
    """L√™ todos os .txt do diret√≥rio"""
    print(f"{Fore.BLUE}üìÇ Lendo arquivos .txt de: {folder}")
    
    if not os.path.exists(folder):
        print(f"{Fore.RED}‚ùå Diret√≥rio n√£o encontrado: {folder}")
        return {}
    
    data: dict[str, str] = {}
    txt_files = []
    
    for root, _, files in os.walk(folder):
        for f in files:
            if f.lower().endswith(".txt"):
                txt_files.append(os.path.join(root, f))
    
    if not txt_files:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Nenhum arquivo .txt encontrado em {folder}")
        return {}
    
    print(f"{Fore.GREEN}üìÑ Encontrados {len(txt_files)} arquivos .txt")
    
    for fp in tqdm(txt_files, desc="Lendo arquivos"):
        try:
            with open(fp, "r", encoding="utf-8") as fh:
                content = fh.read().strip()
            if content:
                data[fp] = content
                print(f"{Fore.GREEN}  ‚úÖ {os.path.basename(fp)} ({len(content)} caracteres)")
            else:
                print(f"{Fore.YELLOW}  ‚ö†Ô∏è {os.path.basename(fp)} est√° vazio")
        except Exception as e:
            print(f"{Fore.RED}  ‚ùå Erro ao ler {os.path.basename(fp)}: {e}")
    
    return data

def chunk_text(text: str, chunk_size: int = 800, overlap: int = 120) -> list[str]:
    """Quebra texto em chunks com sobreposi√ß√£o"""
    chunks = []
    start = 0
    n = len(text)
    
    while start < n:
        end = min(start + chunk_size, n)
        chunks.append(text[start:end])
        if end == n:
            break
        start = end - overlap
        if start < 0:
            start = 0
    
    return chunks

# ================================
# Menu Principal
# ================================

def show_main_menu():
    """Exibe o menu principal"""
    print(f"\n{Fore.CYAN}üìã MENU PRINCIPAL")
    print(f"{Fore.CYAN}{'='*50}")
    
    print_menu_option(1, "üèóÔ∏è Construir Base de Conhecimento", 
                     "Processa seus arquivos .txt e cria o √≠ndice de busca")
    
    print_menu_option(2, "üí¨ Chat Interativo Inteligente", 
                     "Conversa inteligente com IA que guia quando necess√°rio")
    
    print_menu_option(3, "üìä Verificar Status do Sistema", 
                     "Mostra informa√ß√µes sobre arquivos e configura√ß√µes")
    
    print_menu_option(4, "‚öôÔ∏è Configura√ß√µes", 
                     "Ajustar par√¢metros b√°sicos do sistema")
    
    print_menu_option(5, "üìö Ajuda", 
                     "Guias, exemplos e solu√ß√£o de problemas")
    
    print_menu_option(6, "üö™ Sair", 
                     "Encerra o programa")

# ================================
# Sistema de Status
# ================================

def check_system_status():
    """Verifica e mostra status do sistema"""
    print(f"\n{Fore.GREEN}üìä STATUS DO SISTEMA")
    print(f"{Fore.GREEN}{'='*40}")
    
    config = DEFAULT_CONFIG
    
    # Verificar diret√≥rios
    print(f"\n{Fore.CYAN}üìÅ Diret√≥rios:")
    dirs_to_check = [
        ("Data (documentos)", config["docs_path"]),
        ("Index (√≠ndices)", os.path.dirname(config["index_path"])),
        ("History (hist√≥rico)", os.path.dirname(config["history_path"]))
    ]
    
    for name, path in dirs_to_check:
        if os.path.exists(path):
            print(f"  ‚úÖ {name}: {path}")
        else:
            print(f"  ‚ùå {name}: {path} (n√£o encontrado)")
    
    # Verificar arquivos de dados
    print(f"\n{Fore.CYAN}üìÑ Arquivos de dados:")
    if os.path.exists(config["docs_path"]):
        txt_files = [f for f in os.listdir(config["docs_path"]) if f.endswith('.txt')]
        if txt_files:
            print(f"  ‚úÖ {len(txt_files)} arquivos .txt encontrados:")
            for f in txt_files[:5]:  # Mostrar apenas os primeiros 5
                print(f"    üìù {f}")
            if len(txt_files) > 5:
                print(f"    ... e mais {len(txt_files) - 5} arquivos")
        else:
            print(f"  ‚ö†Ô∏è Nenhum arquivo .txt encontrado em {config['docs_path']}")
    else:
        print(f"  ‚ùå Diret√≥rio {config['docs_path']} n√£o existe")
    
    # Verificar √≠ndice
    print(f"\n{Fore.CYAN}üîç √çndice de busca:")
    if os.path.exists(config["index_path"]):
        print(f"  ‚úÖ √çndice FAISS: {config['index_path']}")
        try:
            index = faiss.read_index(config["index_path"])
            print(f"  üìä Vetores no √≠ndice: {index.ntotal}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao ler √≠ndice: {e}")
    else:
        print(f"  ‚ùå √çndice n√£o encontrado: {config['index_path']}")
    
    if os.path.exists(config["meta_path"]):
        print(f"  ‚úÖ Metadados: {config['meta_path']}")
        try:
            with open(config["meta_path"], 'r', encoding='utf-8') as f:
                lines = sum(1 for _ in f)
            print(f"  üìä Chunks de texto: {lines}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao ler metadados: {e}")
    else:
        print(f"  ‚ùå Metadados n√£o encontrados: {config['meta_path']}")
    
    # Verificar configura√ß√µes
    print(f"\n{Fore.CYAN}‚öôÔ∏è Configura√ß√µes:")
    if os.path.exists(config["settings_path"]):
        print(f"  ‚úÖ Arquivo de configura√ß√µes existe")
        try:
            with open(config["settings_path"], 'r', encoding='utf-8') as f:
                settings = json.load(f)
            print(f"  üìä Modelo de embeddings: {settings.get('embedding_model', 'N/A')}")
            print(f"  üìä Dimens√£o: {settings.get('dimension', 'N/A')}")
            print(f"  üìä Criado em: {settings.get('created_at', 'N/A')}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao ler configura√ß√µes: {e}")
    else:
        print(f"  ‚ö†Ô∏è Usando configura√ß√µes padr√£o")
    
    # Recomenda√ß√µes
    print(f"\n{Fore.YELLOW}üí° Recomenda√ß√µes:")
    if not os.path.exists(config["docs_path"]):
        print("  ‚Ä¢ Crie o diret√≥rio 'data' e adicione arquivos .txt")
    elif not os.path.exists(config["index_path"]):
        print("  ‚Ä¢ Execute 'Construir Base de Conhecimento' (op√ß√£o 1)")
    else:
        print("  ‚Ä¢ Sistema pronto! Use 'Iniciar Chat' (op√ß√£o 2)")
    
    wait_for_enter()

# ================================
# Constru√ß√£o de Base de Conhecimento
# ================================

def build_knowledge_base():
    """Constr√≥i a base de conhecimento de forma interativa"""
    print(f"\n{Fore.GREEN}üèóÔ∏è CONSTRUINDO BASE DE CONHECIMENTO")
    print(f"{Fore.GREEN}{'='*50}")
    
    config = DEFAULT_CONFIG.copy()
    
    # Verificar se j√° existe uma base de conhecimento
    base_exists = (os.path.exists(config["index_path"]) and 
                   os.path.exists(config["meta_path"]) and 
                   os.path.exists(config["settings_path"]))
    
    if base_exists:
        print(f"\n{Fore.YELLOW}‚ö†Ô∏è BASE DE CONHECIMENTO J√Å EXISTE!")
        print(f"{Fore.CYAN}üìä Arquivos encontrados:")
        print(f"  ‚úÖ √çndice FAISS: {config['index_path']}")
        print(f"  ‚úÖ Metadados: {config['meta_path']}")
        print(f"  ‚úÖ Configura√ß√µes: {config['settings_path']}")
        
        # Mostrar informa√ß√µes da base atual
        try:
            with open(config["settings_path"], "r", encoding="utf-8") as f:
                settings = json.load(f)
            print(f"\n{Fore.CYAN}üìã Informa√ß√µes da base atual:")
            print(f"  üìÑ Total de documentos: {settings.get('total_documents', 'N/A')}")
            print(f"  üìù Total de chunks: {settings.get('total_chunks', 'N/A')}")
            print(f"  üìÖ Criado em: {settings.get('created_at', 'N/A')}")
            print(f"  üß† Modelo: {settings.get('embedding_model', 'N/A')}")
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erro ao ler configura√ß√µes: {e}")
        
        print(f"\n{Fore.YELLOW}üîÑ OP√á√ïES DISPON√çVEIS:")
        print(f"  1Ô∏è‚É£ Reconstruir base (substituir atual)")
        print(f"  2Ô∏è‚É£ Manter base atual (cancelar)")
        
        rebuild_choice = get_user_choice(2)
        
        if rebuild_choice == 2:
            print(f"\n{Fore.GREEN}‚úÖ Opera√ß√£o cancelada. Base atual mantida.")
            wait_for_enter()
            return
        
        print(f"\n{Fore.YELLOW}üóëÔ∏è Removendo base antiga...")
        try:
            if os.path.exists(config["index_path"]):
                os.remove(config["index_path"])
                print(f"  ‚úÖ √çndice removido")
            if os.path.exists(config["meta_path"]):
                os.remove(config["meta_path"])
                print(f"  ‚úÖ Metadados removidos")
            if os.path.exists(config["settings_path"]):
                os.remove(config["settings_path"])
                print(f"  ‚úÖ Configura√ß√µes removidas")
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erro ao remover arquivos: {e}")
            wait_for_enter()
            return
        
        print(f"{Fore.GREEN}‚úÖ Base anterior removida com sucesso!")
        print(f"\n{Fore.GREEN}üèóÔ∏è RECONSTRUINDO BASE DE CONHECIMENTO...")
    
    # Verificar se diret√≥rio data existe
    if not os.path.exists(config["docs_path"]):
        print(f"{Fore.RED}‚ùå Diret√≥rio 'data' n√£o encontrado!")
        if confirm_action("Deseja criar o diret√≥rio 'data'?"):
            os.makedirs(config["docs_path"], exist_ok=True)
            print(f"{Fore.GREEN}‚úÖ Diret√≥rio 'data' criado!")
            print(f"{Fore.YELLOW}üí° Adicione seus arquivos .txt em '{config['docs_path']}' e tente novamente")
            wait_for_enter()
            return
        else:
            return
    
    # Ler arquivos
    documents = read_text_files(config["docs_path"])
    if not documents:
        print(f"{Fore.RED}‚ùå Nenhum arquivo .txt encontrado ou todos est√£o vazios!")
        print(f"{Fore.YELLOW}üí° Adicione arquivos .txt em '{config['docs_path']}' e tente novamente")
        wait_for_enter()
        return
    
    print(f"\n{Fore.CYAN}üìä RESUMO DOS ARQUIVOS:")
    total_chars = 0
    for filepath, content in documents.items():
        filename = os.path.basename(filepath)
        char_count = len(content)
        total_chars += char_count
        print(f"  üìÑ {filename}: {char_count:,} caracteres")
    
    print(f"\n{Fore.GREEN}‚úÖ Total: {len(documents)} arquivos, {total_chars:,} caracteres")
    
    if not confirm_action("Continuar com a constru√ß√£o do √≠ndice?"):
        return
    
    try:
        # Criar chunks
        print(f"\n{Fore.BLUE}üìù Dividindo textos em chunks...")
        chunks: list[str] = []
        metadatas: list[Metadata] = []
        
        for filepath, content in documents.items():
            print(f"  üìù Processando {os.path.basename(filepath)}...")
            file_chunks = chunk_text(content, config["chunk_size"], config["overlap"])
            print(f"    üìä {len(content)} chars ‚Üí {len(file_chunks)} chunks")
            
            start_char = 0
            for i, chunk in enumerate(file_chunks):
                chunks.append(chunk)
                end_char = start_char + len(chunk)
                metadatas.append(Metadata(
                    source=filepath,
                    chunk_id=i,
                    start_char=start_char,
                    end_char=end_char
                ))
                start_char = end_char - config["overlap"]
        
        print(f"{Fore.GREEN}‚úÖ Criados {len(chunks)} chunks")
        
        # Criar embeddings
        print(f"\n{Fore.BLUE}üß† Carregando modelo de embeddings...")
        print(f"  üì¶ Modelo: {config['embedding_model']}")
        model = SentenceTransformer(config["embedding_model"])
        
        print(f"{Fore.BLUE}üîÑ Gerando embeddings...")
        embeddings = []
        batch_size = 32
        
        for i in tqdm(range(0, len(chunks), batch_size), desc="Processando chunks"):
            batch = chunks[i:i+batch_size]
            batch_embeddings = model.encode(batch, show_progress_bar=False)
            embeddings.extend(batch_embeddings)
        
        embeddings_array = np.array(embeddings).astype('float32')
        print(f"{Fore.GREEN}‚úÖ Embeddings criados: {embeddings_array.shape}")
        
        # Criar √≠ndice FAISS
        print(f"\n{Fore.BLUE}üîç Construindo √≠ndice FAISS...")
        dimension = embeddings_array.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
        
        # Normalizar embeddings para cosine similarity
        faiss.normalize_L2(embeddings_array)
        index.add(embeddings_array)
        
        print(f"{Fore.GREEN}‚úÖ √çndice constru√≠do com {index.ntotal} vetores")
        
        # Criar diret√≥rios de sa√≠da
        os.makedirs(os.path.dirname(config["index_path"]), exist_ok=True)
        
        # Salvar √≠ndice
        print(f"\n{Fore.BLUE}üíæ Salvando arquivos...")
        faiss.write_index(index, config["index_path"])
        print(f"‚úÖ √çndice salvo em: {config['index_path']}")
        
        # Salvar metadados
        with open(config["meta_path"], "w", encoding="utf-8") as f:
            for i, (chunk, meta) in enumerate(zip(chunks, metadatas)):
                data = {
                    "chunk_id": i,
                    "text": chunk,
                    "source": meta.source,
                    "start_char": meta.start_char,
                    "end_char": meta.end_char
                }
                f.write(json.dumps(data, ensure_ascii=False) + "\n")
        print(f"‚úÖ Metadados salvos em: {config['meta_path']}")
        
        # Salvar configura√ß√µes
        settings = {
            "embedding_model": config["embedding_model"],
            "chunk_size": config["chunk_size"],
            "overlap": config["overlap"],
            "dimension": dimension,
            "total_chunks": len(chunks),
            "total_documents": len(documents),
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(config["settings_path"], "w", encoding="utf-8") as f:
            json.dump(settings, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Configura√ß√µes salvas em: {config['settings_path']}")
        
        print(f"\n{Fore.GREEN}üéâ BASE DE CONHECIMENTO CRIADA COM SUCESSO!")
        print(f"{Fore.GREEN}{'='*50}")
        print(f"{Fore.CYAN}üìä Estat√≠sticas:")
        print(f"  üìÑ Documentos processados: {len(documents)}")
        print(f"  üìù Chunks criados: {len(chunks)}")
        print(f"  üß† Dimens√£o dos embeddings: {dimension}")
        print(f"  üíæ Tamanho do √≠ndice: {index.ntotal} vetores")
        print(f"\n{Fore.YELLOW}üí° Agora voc√™ pode usar a op√ß√£o 2 para conversar com o chatbot!")
        
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Erro durante a constru√ß√£o da base:")
        print(f"{Fore.RED}   {str(e)}")
        print(f"\n{Fore.YELLOW}üí° Tente novamente ou consulte a ajuda (op√ß√£o 5)")
    
    wait_for_enter()

# ================================
# Chat Interativo
# ================================

def load_meta(meta_path: str) -> list[dict[str, Any]]:
    """Carrega metadados do arquivo JSONL"""
    items = []
    with open(meta_path, "r", encoding="utf-8") as f:
        for line in f:
            items.append(json.loads(line))
    return items

def search_index(query: str, index_path: str, meta_path: str, top_k: int = 3) -> list[Retrieved]:
    """Busca no √≠ndice FAISS"""
    config = DEFAULT_CONFIG
    
    # Carregar √≠ndice e metadados
    index = faiss.read_index(index_path)
    meta = load_meta(meta_path)
    
    # Criar embedding da query
    model = SentenceTransformer(config["embedding_model"])
    query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    
    # Buscar
    scores, indices = index.search(query_embedding, top_k)
    indices = indices[0]
    scores = scores[0]
    
    # Montar resultados
    results: list[Retrieved] = []
    for i, score in zip(indices, scores):
        if i == -1:
            continue
        rec = meta[i]
        metadata = Metadata(
            source=rec["source"],
            chunk_id=rec["chunk_id"],
            start_char=rec["start_char"],
            end_char=rec["end_char"]
        )
        results.append(Retrieved(text=rec["text"], meta=metadata, score=float(score)))
    
    return results

def generate_answer(contexts: list[Retrieved], question: str) -> str:
    """Gera resposta usando FLAN-T5"""
    config = DEFAULT_CONFIG
    
    # Montar prompt
    context_text = "\n\n".join([
        f"[DOCUMENTO {i+1}]\n{ctx.text}" 
        for i, ctx in enumerate(contexts)
    ])
    
    prompt = f"""Voc√™ √© um assistente da ICTA Technology. Responda APENAS com base no contexto fornecido. Se a resposta n√£o estiver no contexto, diga que n√£o sabe e sugira contato com um humano.

CONTEXTO:
{context_text}

PERGUNTA: {question}

RESPOSTA:"""
    
    # Carregar modelo
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = AutoTokenizer.from_pretrained(config["generation_model"])
        model = AutoModelForSeq2SeqLM.from_pretrained(config["generation_model"]).to(device)
        
        # Gerar resposta
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=config["max_tokens"],
                do_sample=True,
                temperature=0.3,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.strip()
        
    except Exception as e:
        return f"Erro ao gerar resposta: {str(e)}"

def print_colored(text: str, color: str = "white"):
    """Imprime texto com cor espec√≠fica"""
    colors = {
        "red": Fore.RED,
        "green": Fore.GREEN,
        "blue": Fore.BLUE,
        "yellow": Fore.YELLOW,
        "cyan": Fore.CYAN,
        "magenta": Fore.MAGENTA,
        "white": Fore.WHITE,
        "gray": Fore.LIGHTBLACK_EX
    }
    print(f"{colors.get(color, Fore.WHITE)}{text}{Style.RESET_ALL}")

def clear_screen():
    """Limpa a tela do terminal"""
    os.system('cls' if os.name == 'nt' else 'clear')

def classify_query_intent(question: str) -> dict:
    """Classifica a inten√ß√£o da pergunta usando FLAN-T5"""
    try:
        config = DEFAULT_CONFIG
        model_name = config["generation_model"]
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
        
        # Prompt para classifica√ß√£o de inten√ß√£o
        classification_prompt = f"""Analise esta pergunta e classifique em uma das categorias:
CATEGORIAS: servicos, integracao, bi, automacao, ia, geral, saudacao, despedida, unclear

Pergunta: "{question}"

Categoria:"""
        
        inputs = tokenizer(classification_prompt, return_tensors="pt", max_length=512, truncation=True).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=50,
                temperature=0.3,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        intent = tokenizer.decode(outputs[0], skip_special_tokens=True).strip().lower()
        
        # Mapear inten√ß√µes para t√≥picos
        intent_mapping = {
            "servicos": "servi√ßos oferecidos pela ICTA",
            "integracao": "integra√ß√µes com TOTVS",
            "bi": "Business Intelligence e relat√≥rios",
            "automacao": "automa√ß√£o de processos",
            "ia": "intelig√™ncia artificial",
            "geral": "informa√ß√µes gerais da empresa",
            "saudacao": "sauda√ß√£o",
            "despedida": "despedida",
            "unclear": "n√£o est√° claro"
        }
        
        return {
            "intent": intent,
            "topic": intent_mapping.get(intent, "informa√ß√µes gerais"),
            "confidence": "high" if intent in intent_mapping else "low"
        }
        
    except Exception as e:
        print_colored(f"‚ùå Erro na classifica√ß√£o: {e}", "red")
        return {"intent": "unclear", "topic": "informa√ß√µes gerais", "confidence": "low"}

def generate_clarification_questions(intent_info: dict, original_question: str) -> str:
    """Gera perguntas de esclarecimento usando FLAN-T5"""
    try:
        config = DEFAULT_CONFIG
        model_name = config["generation_model"]
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
        
        # Prompt para gerar perguntas de esclarecimento
        clarification_prompt = f"""Voc√™ √© um assistente da ICTA Technology, empresa de consultoria em BI, automa√ß√£o e IA.

A pergunta do usu√°rio "{original_question}" n√£o est√° clara sobre o t√≥pico "{intent_info['topic']}".

Fa√ßa 2-3 perguntas espec√≠ficas para ajudar a entender melhor o que o usu√°rio precisa sobre este t√≥pico.

Perguntas de esclarecimento:"""
        
        inputs = tokenizer(clarification_prompt, return_tensors="pt", max_length=512, truncation=True).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        questions = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        return questions
        
    except Exception as e:
        print_colored(f"‚ùå Erro na gera√ß√£o de perguntas: {e}", "red")
        return "Pode me dar mais detalhes sobre o que voc√™ precisa?"

def generate_guided_response(contexts: list, question: str, intent_info: dict) -> str:
    """Gera resposta guiada com intera√ß√£o usando FLAN-T5"""
    try:
        config = DEFAULT_CONFIG
        model_name = config["generation_model"]
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
        
        # Verifica se h√° contextos relevantes
        if not contexts or all(ctx.score < 0.3 for ctx in contexts):
            # Sem contextos relevantes - guiar o usu√°rio
            guidance_prompt = f"""Voc√™ √© um assistente da ICTA Technology.

O usu√°rio perguntou: "{question}"
T√≥pico identificado: {intent_info['topic']}

N√£o encontrei informa√ß√µes espec√≠ficas sobre isso. 

Como assistente especializado em BI, automa√ß√£o e IA, ofere√ßa ajuda alternativa:
1. Sugira t√≥picos relacionados que podem interessar
2. Fa√ßa perguntas para entender melhor a necessidade
3. Ofere√ßa op√ß√µes de contato se necess√°rio

Resposta √∫til:"""
        else:
            # Com contextos - resposta normal melhorada
            context_text = "\n".join([f"- {ctx.text[:200]}..." for ctx in contexts[:3]])
            
            guidance_prompt = f"""Voc√™ √© um assistente especializado da ICTA Technology.

Pergunta: {question}
Contexto encontrado:
{context_text}

Com base no contexto, forne√ßa uma resposta completa e √∫til. Se a informa√ß√£o n√£o for suficiente, fa√ßa perguntas para ajudar melhor o usu√°rio.

Resposta:"""
        
        inputs = tokenizer(guidance_prompt, return_tensors="pt", max_length=512, truncation=True).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=300,
                temperature=0.6,
                do_sample=True,
                repetition_penalty=1.2,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        return response
        
    except Exception as e:
        print_colored(f"‚ùå Erro na gera√ß√£o de resposta: {e}", "red")
        return "Desculpe, houve um erro. Pode reformular sua pergunta?"

def suggest_related_topics(intent: str) -> list[str]:
    """Sugere t√≥picos relacionados baseado na inten√ß√£o"""
    topic_suggestions = {
        "servicos": [
            "Quais s√£o os principais servi√ßos de BI?",
            "Como funciona a automa√ß√£o de processos?",
            "Que tipos de IA voc√™s implementam?"
        ],
        "integracao": [
            "Quais m√≥dulos TOTVS voc√™s integram?",
            "Como √© feita a migra√ß√£o de dados?",
            "Qual o tempo de implementa√ß√£o?"
        ],
        "bi": [
            "Que ferramentas de BI voc√™s usam?",
            "Como criar dashboards personalizados?",
            "Quais relat√≥rios est√£o dispon√≠veis?"
        ],
        "automacao": [
            "Que processos podem ser automatizados?",
            "Qual o ROI da automa√ß√£o?",
            "Como funciona a implementa√ß√£o?"
        ],
        "ia": [
            "Que tipos de IA voc√™s desenvolvem?",
            "Como a IA pode ajudar meu neg√≥cio?",
            "Quais s√£o os casos de uso mais comuns?"
        ]
    }
    
    return topic_suggestions.get(intent, [
        "Quais s√£o os principais servi√ßos da ICTA?",
        "Como posso entrar em contato?",
        "Que tipos de projetos voc√™s fazem?"
    ])

def interactive_chat():
    """Chat interativo melhorado com guias inteligentes"""
    print_colored("\nüí¨ CHAT INTERATIVO ICTA", "cyan")
    print_colored("=" * 50, "cyan")
    
    # Verificar se h√° base de conhecimento
    if not os.path.exists("./index/faiss.index"):
        print_colored("‚ùå Base de conhecimento n√£o encontrada!", "red")
        print_colored("   Execute a op√ß√£o 1 primeiro para construir a base.", "yellow")
        wait_for_enter()
        return
    
    print_colored("ü§ñ Ol√°! Sou o assistente da ICTA Technology.", "green")
    print_colored("üí° Posso ajudar com d√∫vidas sobre BI, automa√ß√£o e IA.", "blue")
    print_colored("üîç Se n√£o souber responder, vou te guiar para encontrar o que precisa!", "blue")
    print_colored("\nüìù Digite sua pergunta (ou 'sair' para encerrar):", "white")
    
    conversation_history = []
    
    while True:
        print_colored("\n" + "‚îÄ" * 50, "gray")
        user_input = input(f"{Fore.YELLOW}üë§ Voc√™: {Style.RESET_ALL}").strip()
        
        if user_input.lower() in ['sair', 'exit', 'quit', 'bye']:
            print_colored("üëã Obrigado por usar o assistente ICTA! At√© logo!", "green")
            break
        
        if not user_input:
            print_colored("‚ùì Por favor, digite uma pergunta.", "yellow")
            continue
        
        # Salvar pergunta do usu√°rio
        conversation_history.append({"role": "user", "content": user_input})
        
        print_colored(f"\nüîç Analisando sua pergunta...", "blue")
        
        # 1. Classificar inten√ß√£o da pergunta
        intent_info = classify_query_intent(user_input)
        
        # 2. Buscar no √≠ndice
        try:
            search_results = search_index(user_input, "./index/faiss.index", "./index/meta.jsonl", top_k=3)
        except Exception as e:
            print_colored(f"‚ùå Erro na busca: {e}", "red")
            search_results = []
        
        # 3. Verificar qualidade dos resultados
        has_good_results = search_results and any(result.score > 0.4 for result in search_results)
        
        if has_good_results:
            # Resposta normal com contexto
            print_colored(f"üí° Encontrei informa√ß√µes relevantes!", "green")
            answer = generate_guided_response(search_results, user_input, intent_info)
        else:
            # Sem resultados bons - modo interativo
            print_colored(f"ü§î Hmm, n√£o encontrei uma resposta espec√≠fica...", "yellow")
            print_colored(f"üìã Identifiquei que voc√™ est√° perguntando sobre: {intent_info['topic']}", "blue")
            
            # Gerar resposta guiada
            answer = generate_guided_response([], user_input, intent_info)
            
            # Adicionar sugest√µes de t√≥picos
            suggestions = suggest_related_topics(intent_info['intent'])
            if suggestions:
                answer += f"\n\nüí° Voc√™ pode me perguntar sobre:\n"
                for i, suggestion in enumerate(suggestions, 1):
                    answer += f"   {i}. {suggestion}\n"
        
        # Exibir resposta
        print_colored(f"\nü§ñ ICTA Assistant:", "cyan")
        print_colored(answer, "white")
        
        # Salvar resposta
        conversation_history.append({"role": "assistant", "content": answer})
        
        # Perguntar se ajudou
        print_colored(f"\n‚ùì Esta resposta foi √∫til? (s/n/mais)", "blue")
        feedback = input(f"{Fore.YELLOW}üìù Feedback: {Style.RESET_ALL}").strip().lower()
        
        if feedback == 'n' or feedback == 'nao':
            print_colored("üîÑ Vou tentar ajudar de outra forma!", "blue")
            clarification = generate_clarification_questions(intent_info, user_input)
            print_colored(f"\nüéØ Para te ajudar melhor:", "cyan")
            print_colored(clarification, "white")
        elif feedback == 'mais':
            print_colored("üîç Pode fazer uma pergunta mais espec√≠fica sobre o mesmo t√≥pico!", "blue")
    
    # Salvar hist√≥rico
    save_conversation_history(conversation_history)
    wait_for_enter()

def save_conversation_history(history: list):
    """Salva o hist√≥rico da conversa"""
    try:
        os.makedirs("./history", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        history_file = f"./history/chat_{timestamp}.json"
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        
        print_colored(f"üíæ Conversa salva em: {history_file}", "green")
    except Exception as e:
        print_colored(f"‚ùå Erro ao salvar hist√≥rico: {e}", "red")

def start_chat():
    """Inicia o chat interativo inteligente"""
    interactive_chat()

# ================================
# Configura√ß√µes
# ================================

def show_settings():
    """Mostra e permite alterar configura√ß√µes b√°sicas"""
    print(f"\n{Fore.GREEN}‚öôÔ∏è CONFIGURA√á√ïES DO SISTEMA")
    print(f"{Fore.GREEN}{'='*40}")
    
    config = DEFAULT_CONFIG
    
    print(f"\n{Fore.CYAN}üìã Configura√ß√µes atuais:")
    print(f"  1. Modelo de embeddings: {config['embedding_model']}")
    print(f"  2. Modelo de gera√ß√£o: {config['generation_model']}")
    print(f"  3. Tamanho do chunk: {config['chunk_size']} caracteres")
    print(f"  4. Sobreposi√ß√£o: {config['overlap']} caracteres")
    print(f"  5. Documentos por busca: {config['top_k']}")
    print(f"  6. Tokens m√°ximos na resposta: {config['max_tokens']}")
    
    print(f"\n{Fore.YELLOW}üí° Dicas:")
    print(f"‚Ä¢ Chunks menores = busca mais precisa, mas pode perder contexto")
    print(f"‚Ä¢ Mais documentos por busca = respostas mais completas")
    print(f"‚Ä¢ Mais tokens = respostas mais longas")
    
    print(f"\n{Fore.LIGHTBLACK_EX}‚ÑπÔ∏è Para alterar configura√ß√µes, edite o arquivo de c√≥digo")
    print(f"‚ÑπÔ∏è Ap√≥s altera√ß√µes, reconstrua a base de conhecimento")
    
    wait_for_enter()

# ================================
# Sistema de Ajuda
# ================================

def show_help():
    """Sistema de ajuda interativo"""
    while True:
        print(f"\n{Fore.CYAN}üìö CENTRAL DE AJUDA")
        print(f"{Fore.CYAN}{'='*40}")
        
        print_menu_option(1, "‚ùì Como come√ßar", 
                         "Primeiros passos para usar o sistema")
        
        print_menu_option(2, "üìÅ Preparar documentos", 
                         "Como organizar seus arquivos .txt")
        
        print_menu_option(3, "üîß Solu√ß√£o de problemas", 
                         "Erros comuns e solu√ß√µes")
        
        print_menu_option(4, "üí° Dicas de uso", 
                         "Como obter melhores resultados")
        
        print_menu_option(5, "üìñ Sobre o projeto", 
                         "Informa√ß√µes t√©cnicas")
        
        print_menu_option(6, "üîô Voltar", 
                         "Retorna ao menu principal")
        
        choice = get_user_choice(6)
        
        if choice == 1:
            show_getting_started()
        elif choice == 2:
            show_document_guide()
        elif choice == 3:
            show_troubleshooting()
        elif choice == 4:
            show_usage_tips()
        elif choice == 5:
            show_about()
        elif choice == 6:
            break

def show_getting_started():
    """Guia de primeiros passos"""
    print(f"\n{Fore.GREEN}üöÄ PRIMEIROS PASSOS")
    print(f"{Fore.GREEN}{'='*30}")
    print(f"""
{Fore.CYAN}Passo 1: Preparar documentos{Style.RESET_ALL}
‚Ä¢ Crie/verifique a pasta 'data' no diret√≥rio do programa
‚Ä¢ Adicione arquivos .txt com suas FAQs e documentos
‚Ä¢ Use formato claro: "P: pergunta R: resposta"

{Fore.CYAN}Passo 2: Construir base de conhecimento{Style.RESET_ALL}
‚Ä¢ No menu principal, escolha op√ß√£o 1
‚Ä¢ Aguarde o processamento dos documentos
‚Ä¢ Isso criar√° um √≠ndice de busca inteligente

{Fore.CYAN}Passo 3: Conversar com o chatbot{Style.RESET_ALL}
‚Ä¢ No menu principal, escolha op√ß√£o 2
‚Ä¢ Digite suas perguntas naturalmente
‚Ä¢ O sistema buscar√° as melhores respostas

{Fore.YELLOW}üéØ Dica: Comece com poucos documentos para testar!{Style.RESET_ALL}
    """)
    wait_for_enter()

def show_document_guide():
    """Guia para prepara√ß√£o de documentos"""
    print(f"\n{Fore.GREEN}üìÅ GUIA DE DOCUMENTOS")
    print(f"{Fore.GREEN}{'='*30}")
    print(f"""
{Fore.CYAN}Estrutura recomendada:{Style.RESET_ALL}
data/
‚îú‚îÄ‚îÄ faq_geral.txt
‚îú‚îÄ‚îÄ produtos.txt
‚îú‚îÄ‚îÄ suporte.txt
‚îî‚îÄ‚îÄ politicas.txt

{Fore.CYAN}Formato dos arquivos .txt:{Style.RESET_ALL}
P: Como funciona o sistema?
R: Nosso sistema utiliza intelig√™ncia artificial...

P: Quais s√£o os pre√ßos?
R: Oferecemos planos a partir de R$ 99/m√™s...

{Fore.CYAN}Boas pr√°ticas:{Style.RESET_ALL}
‚Ä¢ Use linguagem clara e direta
‚Ä¢ Inclua palavras-chave importantes
‚Ä¢ Evite textos muito longos
‚Ä¢ Organize por temas em arquivos separados
‚Ä¢ Teste com perguntas reais dos usu√°rios

{Fore.YELLOW}üí° Exemplo de bom formato:{Style.RESET_ALL}
P: Como entrar em contato?
R: Entre em contato pelo WhatsApp (11) 99999-9999 ou 
email contato@ictatechnology.com. Atendemos de segunda 
a sexta das 9h √†s 18h.
    """)
    wait_for_enter()

def show_troubleshooting():
    """Guia de solu√ß√£o de problemas"""
    print(f"\n{Fore.GREEN}üîß SOLU√á√ÉO DE PROBLEMAS")
    print(f"{Fore.GREEN}{'='*35}")
    print(f"""
{Fore.RED}‚ùå "Nenhum arquivo .txt encontrado"{Style.RESET_ALL}
‚Ä¢ Verifique se a pasta 'data' existe
‚Ä¢ Confirme que h√° arquivos .txt na pasta
‚Ä¢ Verifique se os arquivos n√£o est√£o vazios

{Fore.RED}‚ùå "Modelo n√£o encontrado"{Style.RESET_ALL}
‚Ä¢ Primeira execu√ß√£o precisa de internet
‚Ä¢ Aguarde o download dos modelos (pode demorar)
‚Ä¢ Verifique sua conex√£o com a internet

{Fore.RED}‚ùå "Erro de mem√≥ria"{Style.RESET_ALL}
‚Ä¢ Feche outros programas pesados
‚Ä¢ Use um modelo menor (flan-t5-small)
‚Ä¢ Reduza o tamanho dos chunks no c√≥digo

{Fore.RED}‚ùå "Respostas ruins"{Style.RESET_ALL}
‚Ä¢ Melhore a qualidade dos documentos
‚Ä¢ Use textos mais espec√≠ficos
‚Ä¢ Adicione mais exemplos similares
‚Ä¢ Verifique se as palavras-chave est√£o corretas

{Fore.YELLOW}üÜò Precisa de mais ajuda?{Style.RESET_ALL}
‚Ä¢ GitHub: https://github.com/jesseff20/rag-chatbot
‚Ä¢ Issues: reporte problemas no GitHub
‚Ä¢ Email: contato@ictatechnology.com
    """)
    wait_for_enter()

def show_usage_tips():
    """Dicas de uso avan√ßado"""
    print(f"\n{Fore.GREEN}üí° DICAS DE USO AVAN√áADO")
    print(f"{Fore.GREEN}{'='*35}")
    print(f"""
{Fore.CYAN}üìù Para melhores documentos:{Style.RESET_ALL}
‚Ä¢ Use perguntas que seus clientes realmente fazem
‚Ä¢ Inclua sin√¥nimos e varia√ß√µes
‚Ä¢ Mantenha respostas focadas e diretas
‚Ä¢ Atualize regularmente o conte√∫do

{Fore.CYAN}üîç Para melhores buscas:{Style.RESET_ALL}
‚Ä¢ Use palavras-chave espec√≠ficas
‚Ä¢ Fa√ßa perguntas completas, n√£o apenas palavras
‚Ä¢ Seja espec√≠fico sobre o que quer saber
‚Ä¢ Reformule se n√£o conseguir boa resposta

{Fore.CYAN}‚öôÔ∏è Para melhor performance:{Style.RESET_ALL}
‚Ä¢ Organize documentos por tema
‚Ä¢ Mantenha arquivos com tamanho moderado
‚Ä¢ Remova informa√ß√µes duplicadas
‚Ä¢ Teste regularmente a qualidade

{Fore.CYAN}üéØ Para casos espec√≠ficos:{Style.RESET_ALL}
‚Ä¢ FAQ geral: use linguagem informal
‚Ä¢ Documenta√ß√£o t√©cnica: seja preciso
‚Ä¢ Atendimento: inclua contatos e hor√°rios
‚Ä¢ Produtos: especifica√ß√µes e pre√ßos atuais

{Fore.YELLOW}üîÑ Lembre-se: ap√≥s mudan√ßas nos documentos,{Style.RESET_ALL}
{Fore.YELLOW}reconstrua a base de conhecimento (op√ß√£o 1)!{Style.RESET_ALL}
    """)
    wait_for_enter()

def show_about():
    """Informa√ß√µes sobre o projeto"""
    print(f"\n{Fore.GREEN}üìñ SOBRE O PROJETO")
    print(f"{Fore.GREEN}{'='*30}")
    print(f"""
{Fore.CYAN}RAG Chatbot ICTA Technology{Style.RESET_ALL}
Vers√£o: 2.0 - Interface Simplificada
Data: Agosto 2025

{Fore.CYAN}üë®‚Äçüíª Desenvolvido por:{Style.RESET_ALL}
Jesse Fernandes
ICTA Technology
Email: jesse.fernandes@ictatechnology.com

{Fore.CYAN}üîß Tecnologias utilizadas:{Style.RESET_ALL}
‚Ä¢ FAISS - Busca vetorial Facebook AI
‚Ä¢ Sentence Transformers - Embeddings de texto
‚Ä¢ FLAN-T5 - Modelo de linguagem Google
‚Ä¢ Python 3.8+ - Linguagem de programa√ß√£o

{Fore.CYAN}‚ú® Caracter√≠sticas:{Style.RESET_ALL}
‚Ä¢ 100% local (sem APIs pagas)
‚Ä¢ Interface amig√°vel e interativa
‚Ä¢ Configura√ß√£o autom√°tica
‚Ä¢ Suporte a m√∫ltiplos documentos
‚Ä¢ Hist√≥rico de conversas

{Fore.CYAN}üìÑ Licen√ßa:{Style.RESET_ALL}
MIT License - Uso livre e modifica√ß√£o permitida

{Fore.CYAN}üåê Links importantes:{Style.RESET_ALL}
‚Ä¢ GitHub: https://github.com/jesseff20/rag-chatbot
‚Ä¢ Issues: https://github.com/jesseff20/rag-chatbot/issues
‚Ä¢ Documenta√ß√£o: Arquivo README.md

{Fore.YELLOW}üíù Desenvolvido com ‚ù§Ô∏è para a comunidade!{Style.RESET_ALL}
{Fore.YELLOW}Contribui√ß√µes e sugest√µes s√£o bem-vindas.{Style.RESET_ALL}
    """)
    wait_for_enter()

# ================================
# Fun√ß√£o Principal
# ================================

def main():
    """Fun√ß√£o principal com menu interativo"""
    # Configura√ß√£o inicial
    print_header()
    
    # Loop principal
    while True:
        show_main_menu()
        choice = get_user_choice(6)
        
        if choice == 1:
            build_knowledge_base()
        elif choice == 2:
            start_chat()
        elif choice == 3:
            check_system_status()
        elif choice == 4:
            show_settings()
        elif choice == 5:
            show_help()
        elif choice == 6:
            print(f"\n{Fore.GREEN}üëã Obrigado por usar o RAG Chatbot ICTA!")
            print(f"{Fore.GREEN}At√© logo!")
            break

if __name__ == "__main__":
    main()
