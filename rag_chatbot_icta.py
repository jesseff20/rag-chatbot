#!/usr/bin/env python3
"""
RAG Chatbot ICTA Technology - Vers√£o Simplificada com Menus Interativos
Sistema amig√°vel para cria√ß√£o de chatbot FAQ

Autor: Jesse Fernandes - ICTA Technology
GitHub: https://github.com/jesseff20/rag-chatbot
"""

from __future__ import annotations
import os
import json
import sys
import time
from datetime import datetime
from dataclasses import dataclass
from typing import Any, List, Dict, Optional

# Imports principais
import faiss  # type: ignore
import numpy as np
from tqdm import tqdm
from colorama import Fore, Style, init, Back
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Inicializar colorama para Windows
init(autoreset=True)

# ================================
# Configura√ß√µes Padr√£o
# ================================

DEFAULT_CONFIG = {
    "docs_path": "./data",
    "index_path": "./index/faiss.index",
    "meta_path": "./index/meta.jsonl",
    "settings_path": "./index/settings.json",
    "history_path": "./history/chat_history.jsonl",
    "chunk_size": 600,  # Aumentado para chunks maiores
    "overlap": 120,     # Sobreposi√ß√£o otimizada (20% do chunk_size)
    "top_k": 12,        # Aumentado para recuperar mais contexto relevante
    "max_tokens": 10000,  # Significativamente aumentado para respostas mais completas
    "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # Melhor para portugu√™s
    "generation_model": "google/flan-t5-base",  # Volta para FLAN-T5 que √© mais est√°vel
    "fallback_model": "google/flan-t5-small"  # Fallback menor
}

# ================================
# Classes de Dados
# ================================

@dataclass
class Metadata:
    source: str
    chunk_id: int
    start_char: int
    end_char: int

@dataclass  
class Retrieved:
    text: str
    meta: Metadata
    score: float

# ================================
# Sistema H√≠brido RAG + FLAN-T5
# ================================

class FlanT5Fallback:
    """Sistema de fallback usando FLAN-T5 quando RAG n√£o tem resposta adequada"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_model(self):
        """Carrega o modelo FLAN-T5 para fallback"""
        if self.is_loaded:
            return True
            
        try:
            print(f"{Fore.BLUE}ü§ñ Carregando modelo FLAN-T5 para respostas de fallback...")
            
            # Usar modelo do config
            model_name = DEFAULT_CONFIG["generation_model"]
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            
            # Mover para GPU se dispon√≠vel
            if torch.cuda.is_available():
                self.model = self.model.to(self.device)
                print(f"  üöÄ Modelo carregado na GPU")
            else:
                print(f"  üíª Modelo carregado na CPU")
                
            self.is_loaded = True
            print(f"{Fore.GREEN}‚úÖ FLAN-T5 pronto para fallback!")
            return True
            
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erro ao carregar FLAN-T5: {e}")
            return False
    
    def generate_fallback_response(self, question: str, context_type: str = "") -> str:
        """Gera resposta usando FLAN-T5 quando RAG n√£o tem resposta"""
        if not self.is_loaded:
            if not self.load_model():
                return "Desculpe, n√£o consegui processar sua pergunta no momento. Tente novamente mais tarde."
        
        try:
            # Criar prompt contextualizado para ICTA
            icta_context = """A ICTA Technology √© uma consultoria especializada em:
‚Ä¢ Business Intelligence (BI) e Analytics
‚Ä¢ Automa√ß√£o de processos e RPA
‚Ä¢ Intelig√™ncia Artificial e Machine Learning
‚Ä¢ Integra√ß√µes com sistemas ERP (especialmente TOTVS)
‚Ä¢ Transforma√ß√£o digital e otimiza√ß√£o de dados

Principais servi√ßos:
- Dashboards e relat√≥rios executivos
- Automa√ß√£o de workflows
- Chatbots e assistentes virtuais
- Migra√ß√£o e integra√ß√£o de dados
- Consultoria em arquitetura de dados"""

            prompt = f"""Voc√™ √© um assistente especializado da ICTA Technology. Responda de forma √∫til e profissional.

CONTEXTO DA EMPRESA:
{icta_context}

PERGUNTA DO CLIENTE: {question}

INSTRU√á√ïES:
- Seja profissional e √∫til
- Use informa√ß√µes gerais sobre tecnologia quando apropriado
- Se n√£o souber algo espec√≠fico da ICTA, seja honesto
- Sugira entrar em contato para informa√ß√µes detalhadas
- Mantenha foco em BI, automa√ß√£o, IA e integra√ß√µes

RESPOSTA:"""

            # Tokenizar e gerar resposta
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                max_length=600, 
                truncation=True
            )
            
            # Mover inputs para dispositivo correto
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=800,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Limpar resposta (remover prompt repetido)
            response = response.replace(prompt, "").strip()
            
            # Adicionar disclaimer sobre ser resposta geral
            disclaimer = "\n\nüí° *Resposta gerada por IA geral. Para informa√ß√µes espec√≠ficas da ICTA, entre em contato conosco.*"
            
            return response + disclaimer
            
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erro ao gerar resposta com FLAN-T5: {e}")
            return f"Desculpe, tive um problema t√©cnico. Para melhor atendimento, entre em contato diretamente com nossa equipe da ICTA Technology."

# Inst√¢ncia global do sistema de fallback
flan_fallback = FlanT5Fallback()

class PortugueseLLM:
    """FLAN-T5 otimizado para portugu√™s com prompts melhorados"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    def load_model(self) -> bool:
        """Carrega o modelo FLAN-T5 otimizado para portugu√™s"""
        try:
            model_name = DEFAULT_CONFIG["generation_model"]
            print(f"ü§ñ Carregando FLAN-T5 otimizado: {model_name}")
            
            # Usar FLAN-T5 que √© mais est√°vel
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            
            # Configurar padding token se n√£o existir
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Mover para dispositivo apropriado
            if self.device == "cuda" and torch.cuda.is_available():
                try:
                    self.model = self.model.to(self.device)
                    print(f"  üöÄ Modelo carregado na GPU")
                except:
                    self.device = "cpu"
                    print(f"  üíª Modelo carregado na CPU (GPU n√£o dispon√≠vel)")
            else:
                print(f"  üíª Modelo carregado na CPU")
            
            self.is_loaded = True
            print("‚úÖ FLAN-T5 otimizado carregado com sucesso!")
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar FLAN-T5: {e}")
            self.is_loaded = False
            return False
    
    def generate_enhanced_response(self, question: str, rag_context: str = "") -> str:
        """Gera resposta usando FLAN-T5 com prompts otimizados para portugu√™s"""
        try:
            if not self.is_loaded:
                success = self.load_model()
                if not success:
                    return "Erro ao carregar modelo. Tente novamente."
            
            # Criar prompt otimizado para FLAN-T5 em portugu√™s
            if rag_context:
                # Limitar e limpar contexto RAG
                context_lines = rag_context.split('\n')[:2]  # Usar apenas 2 linhas
                clean_context = ' '.join(context_lines).strip()
                clean_context = clean_context.replace('[TAGS:', '').replace(']', '')
                
                prompt = f"""Baseado nas informa√ß√µes da ICTA Technology, responda de forma clara e profissional em portugu√™s.

Informa√ß√µes: {clean_context}

Pergunta: {question}

Resposta clara em portugu√™s:"""
            else:
                # Prompt para perguntas gerais
                prompt = f"""Responda como assistente profissional da ICTA Technology em portugu√™s.

A ICTA Technology √© especializada em Business Intelligence, automa√ß√£o de processos e intelig√™ncia artificial para empresas.

Pergunta: {question}

Resposta profissional:"""
            
            # Tokenizar com limite adequado
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=400,
                truncation=True,
                padding=True
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Gerar resposta com par√¢metros otimizados para FLAN-T5
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100,  # Quantidade adequada para FLAN-T5
                    temperature=0.3,     # Conservador para informa√ß√µes espec√≠ficas
                    do_sample=True,
                    top_p=0.8,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )
            
            # Decodificar resposta
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extrair apenas a resposta nova
            if "Resposta clara em portugu√™s:" in response:
                response = response.split("Resposta clara em portugu√™s:")[-1].strip()
            elif "Resposta profissional:" in response:
                response = response.split("Resposta profissional:")[-1].strip()
            else:
                # Remover o prompt original
                response = response.replace(prompt, "").strip()
            
            # Limpeza e valida√ß√£o final
            if len(response) < 15 or len(response) > 400:
                if rag_context:
                    # Usar resposta direta do RAG se dispon√≠vel
                    return clean_context if clean_context else "Entre em contato para mais informa√ß√µes sobre os servi√ßos da ICTA Technology."
                else:
                    return "A ICTA Technology oferece solu√ß√µes em Business Intelligence, automa√ß√£o e intelig√™ncia artificial. Entre em contato para mais informa√ß√µes."
            
            return response
            
        except Exception as e:
            print(f"Erro no modelo FLAN-T5: {e}")
            if rag_context:
                # Fallback para contexto RAG direto
                clean_context = rag_context.replace('[TAGS:', '').replace(']', '').strip()
                return clean_context.split('\n')[0] if clean_context else "Entre em contato para mais informa√ß√µes."
            return "Para informa√ß√µes espec√≠ficas, entre em contato com a ICTA Technology."

# Inst√¢ncia global do modelo portugu√™s
portuguese_llm = PortugueseLLM()

# ================================
# Utilit√°rios de Interface
# ================================

def print_header():
    """Imprime cabe√ßalho do programa"""
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}ü§ñ RAG Chatbot ICTA Technology - Vers√£o Simplificada")
    print(f"{Fore.CYAN}{'='*60}")
    print(f"{Fore.GREEN}Sistema interativo para cria√ß√£o de chatbot FAQ")
    print(f"{Fore.GREEN}GitHub: https://github.com/jesseff20/rag-chatbot")
    print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}\n")

def print_menu_option(number: int, title: str, description: str):
    """Imprime uma op√ß√£o do menu formatada"""
    print(f"{Fore.YELLOW}{number:2}. {Fore.WHITE}{title}")
    print(f"    {Fore.LIGHTBLACK_EX}{description}{Style.RESET_ALL}")

def get_user_choice(max_option: int) -> int:
    """Obt√©m escolha do usu√°rio com valida√ß√£o"""
    while True:
        try:
            choice = input(f"\n{Fore.CYAN}üéØ Escolha uma op√ß√£o (1-{max_option}): {Style.RESET_ALL}")
            choice_num = int(choice)
            if 1 <= choice_num <= max_option:
                return choice_num
            else:
                print(f"{Fore.RED}‚ùå Por favor, escolha um n√∫mero entre 1 e {max_option}")
        except ValueError:
            print(f"{Fore.RED}‚ùå Por favor, digite um n√∫mero v√°lido")
        except KeyboardInterrupt:
            print(f"\n{Fore.YELLOW}üëã Saindo do programa...")
            sys.exit(0)

def confirm_action(message: str) -> bool:
    """Confirma uma a√ß√£o com o usu√°rio"""
    while True:
        response = input(f"{Fore.YELLOW}‚ùì {message} (s/n): {Style.RESET_ALL}").lower().strip()
        if response in ['s', 'sim', 'y', 'yes']:
            return True
        elif response in ['n', 'nao', 'n√£o', 'no']:
            return False
        else:
            print(f"{Fore.RED}‚ùå Digite 's' para sim ou 'n' para n√£o")

def wait_for_enter():
    """Espera o usu√°rio pressionar Enter"""
    _ = input(f"\n{Fore.LIGHTBLACK_EX}üìù Pressione Enter para continuar...{Style.RESET_ALL}")

# ================================
# Utilit√°rios de Processamento
# ================================

def read_jsonl_files(folder: str) -> dict[str, str]:
    """L√™ todos os .jsonl do diret√≥rio com processamento inteligente"""
    print(f"{Fore.BLUE}üìÇ Lendo arquivos .jsonl de: {folder}")
    
    if not os.path.exists(folder):
        print(f"{Fore.RED}‚ùå Diret√≥rio n√£o encontrado: {folder}")
        return {}
    
    data: dict[str, str] = {}
    jsonl_files = []
    
    for root, _, files in os.walk(folder):
        for f in files:
            if f.lower().endswith(".jsonl"):
                jsonl_files.append(os.path.join(root, f))
    
    if not jsonl_files:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Nenhum arquivo .jsonl encontrado em {folder}")
        return {}
    
    print(f"{Fore.GREEN}üìÑ Encontrados {len(jsonl_files)} arquivos .jsonl")
    
    for fp in tqdm(jsonl_files, desc="Lendo arquivos"):
        try:
            content_parts = []
            with open(fp, "r", encoding="utf-8") as fh:
                for line in fh:
                    line = line.strip()
                    if line:
                        entry = json.loads(line)
                        # Processa e enriquece cada entrada
                        processed_entry = process_jsonl_entry(entry, os.path.basename(fp))
                        if processed_entry:
                            content_parts.append(processed_entry)
                        
            if content_parts:
                content = "\n\n".join(content_parts)
                data[fp] = content
                print(f"{Fore.GREEN}  ‚úÖ {os.path.basename(fp)} ({len(content_parts)} entradas, {len(content)} caracteres)")
            else:
                print(f"{Fore.YELLOW}  ‚ö†Ô∏è {os.path.basename(fp)} est√° vazio")
        except Exception as e:
            print(f"{Fore.RED}  ‚ùå Erro ao ler {os.path.basename(fp)}: {e}")
    
    return data

def process_jsonl_entry(entry: dict, filename: str) -> str:
    """Processa uma entrada JSONL para enriquecer o contexto"""
    
    # Determina o contexto baseado no nome do arquivo
    context_prefix = ""
    if "cortesia" in filename.lower() or "saudacao" in filename.lower():
        context_prefix = "[SAUDA√á√ÉO/CORTESIA] "
    elif "empresa" in filename.lower() or "contato" in filename.lower():
        context_prefix = "[EMPRESA/CONTATO] "
    elif "faq" in filename.lower():
        context_prefix = "[FAQ GERAL] "
    elif "servicos" in filename.lower() or "bi" in filename.lower() or "automacao" in filename.lower():
        context_prefix = "[SERVI√áOS/BI/AUTOMA√á√ÉO] "
    elif "integracao" in filename.lower() or "totvs" in filename.lower():
        context_prefix = "[INTEGRA√á√ÉO/TOTVS] "
    elif "politica" in filename.lower():
        context_prefix = "[POL√çTICA/DIRETRIZES] "
    
    # Processa diferentes estruturas de entrada
    processed_text = ""
    
    if 'question' in entry and 'answer' in entry:
        # Formato completo com pergunta e resposta
        processed_text = f"PERGUNTA: {entry['question']}\nRESPOSTA: {entry['answer']}"
    elif 'answer' in entry:
        # Apenas resposta - tenta inferir o contexto
        answer = entry['answer'].strip()
        
        # Adiciona contexto sem√¢ntico baseado no conte√∫do
        if any(word in answer.lower() for word in ['bom dia', 'boa tarde', 'boa noite', 'ol√°', 'oi']):
            context_prefix = "[SAUDA√á√ÉO] "
        elif any(word in answer.lower() for word in ['pre√ßo', 'custo', 'investimento', 'valor']):
            context_prefix = "[PRE√áOS/COMERCIAL] "
        elif any(word in answer.lower() for word in ['totvs', 'erp', 'integra√ß√£o']):
            context_prefix = "[INTEGRA√á√ÉO/ERP] "
        elif any(word in answer.lower() for word in ['bi', 'business intelligence', 'dashboard', 'relat√≥rio']):
            context_prefix = "[BUSINESS INTELLIGENCE] "
        elif any(word in answer.lower() for word in ['automa√ß√£o', 'rpa', 'processo']):
            context_prefix = "[AUTOMA√á√ÉO] "
        elif any(word in answer.lower() for word in ['ia', 'intelig√™ncia artificial', 'chatbot', 'ai']):
            context_prefix = "[INTELIG√äNCIA ARTIFICIAL] "
        elif any(word in answer.lower() for word in ['contato', 'telefone', 'email', 'endere√ßo']):
            context_prefix = "[CONTATO/LOCALIZA√á√ÉO] "
        
        processed_text = f"CONTE√öDO: {answer}"
    elif 'text' in entry:
        processed_text = f"TEXTO: {entry['text']}"
    elif isinstance(entry, str):
        processed_text = f"INFORMA√á√ÉO: {entry}"
    else:
        return ""
    
    # Adiciona metadados para melhor recupera√ß√£o
    enhanced_text = f"{context_prefix}{processed_text}"
    
    # Adiciona palavras-chave relevantes para ICTA
    keywords = []
    text_lower = processed_text.lower()
    
    # Palavras-chave t√©cnicas
    if any(word in text_lower for word in ['dashboard', 'relat√≥rio', 'an√°lise', 'dados']):
        keywords.append("business_intelligence")
    if any(word in text_lower for word in ['automa√ß√£o', 'automatizar', 'processo']):
        keywords.append("automacao_processos")
    if any(word in text_lower for word in ['totvs', 'erp', 'protheus']):
        keywords.append("integracao_erp")
    if any(word in text_lower for word in ['chatbot', 'ia', 'intelig√™ncia']):
        keywords.append("inteligencia_artificial")
    if any(word in text_lower for word in ['python', 'sql', 'api']):
        keywords.append("tecnologia")
    
    if keywords:
        enhanced_text += f" [TAGS: {', '.join(keywords)}]"
    
    return enhanced_text

def chunk_text(text: str, chunk_size: int = 400, overlap: int = 80) -> list[str]:
    """Quebra texto em chunks inteligentes com sobreposi√ß√£o otimizada"""
    import re
    
    chunks = []
    
    # Primeiro, tenta quebrar por senten√ßas completas
    sentences = re.split(r'(?<=[.!?])\s+', text)
    if not sentences:
        sentences = [text]
    
    current_chunk = ""
    
    for sentence in sentences:
        # Se adicionar a pr√≥xima senten√ßa n√£o exceder o limite
        if len(current_chunk + " " + sentence) <= chunk_size:
            if current_chunk:
                current_chunk += " " + sentence
            else:
                current_chunk = sentence
        else:
            # Se o chunk atual n√£o est√° vazio, salva
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            
            # Se a senten√ßa √© muito longa, quebra por caracteres
            if len(sentence) > chunk_size:
                # Quebra a senten√ßa longa mantendo palavras inteiras
                words = sentence.split()
                temp_chunk = ""
                for word in words:
                    if len(temp_chunk + " " + word) <= chunk_size:
                        if temp_chunk:
                            temp_chunk += " " + word
                        else:
                            temp_chunk = word
                    else:
                        if temp_chunk.strip():
                            chunks.append(temp_chunk.strip())
                        temp_chunk = word
                
                current_chunk = temp_chunk
            else:
                current_chunk = sentence
    
    # Adiciona o √∫ltimo chunk se n√£o estiver vazio
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # Se ainda n√£o temos chunks, faz quebra simples por caracteres
    if not chunks and text.strip():
        start = 0
        n = len(text)
        while start < n:
            end = min(start + chunk_size, n)
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            if end == n:
                break
            start = end - overlap
            if start < 0:
                start = 0
    
    # Adiciona sobreposi√ß√£o inteligente entre chunks adjacentes
    enhanced_chunks = []
    for i, chunk in enumerate(chunks):
        if i == 0:
            enhanced_chunks.append(chunk)
        else:
            # Adiciona sobreposi√ß√£o com o chunk anterior
            prev_chunk = chunks[i-1]
            overlap_text = prev_chunk[-overlap:] if len(prev_chunk) > overlap else prev_chunk
            
            # Remove pontua√ß√£o quebrada no in√≠cio da sobreposi√ß√£o
            overlap_text = re.sub(r'^[^\w\s]*', '', overlap_text)
            
            if overlap_text.strip() and not chunk.startswith(overlap_text.strip()):
                enhanced_chunk = overlap_text.strip() + " " + chunk
            else:
                enhanced_chunk = chunk
            
            enhanced_chunks.append(enhanced_chunk)
    
    return enhanced_chunks

# ================================
# Menu Principal
# ================================

def show_main_menu():
    """Exibe o menu principal"""
    print(f"\n{Fore.CYAN}üìã MENU PRINCIPAL")
    print(f"{Fore.CYAN}{'='*50}")
    
    print_menu_option(1, "üèóÔ∏è Construir Base de Conhecimento", 
                     "Processa seus arquivos .jsonl e cria o √≠ndice de busca")
    
    print_menu_option(2, "üí¨ Chat Interativo Inteligente", 
                     "Conversa inteligente com IA que guia quando necess√°rio")
    
    print_menu_option(3, "üìä Verificar Status do Sistema", 
                     "Mostra informa√ß√µes sobre arquivos e configura√ß√µes")
    
    print_menu_option(4, "‚öôÔ∏è Configura√ß√µes", 
                     "Ajustar par√¢metros b√°sicos do sistema")
    
    print_menu_option(5, "üìö Ajuda", 
                     "Guias, exemplos e solu√ß√£o de problemas")
    
    print_menu_option(6, "üö™ Sair", 
                     "Encerra o programa")

# ================================
# Sistema de Status
# ================================

def check_system_status():
    """Verifica e mostra status do sistema"""
    print(f"\n{Fore.GREEN}üìä STATUS DO SISTEMA")
    print(f"{Fore.GREEN}{'='*40}")
    
    config = DEFAULT_CONFIG
    
    # Verificar diret√≥rios
    print(f"\n{Fore.CYAN}üìÅ Diret√≥rios:")
    dirs_to_check = [
        ("Data (documentos)", config["docs_path"]),
        ("Index (√≠ndices)", os.path.dirname(config["index_path"])),
        ("History (hist√≥rico)", os.path.dirname(config["history_path"]))
    ]
    
    for name, path in dirs_to_check:
        if os.path.exists(path):
            print(f"  ‚úÖ {name}: {path}")
        else:
            print(f"  ‚ùå {name}: {path} (n√£o encontrado)")
    
    # Verificar arquivos de dados
    print(f"\n{Fore.CYAN}üìÑ Arquivos de dados:")
    if os.path.exists(config["docs_path"]):
        jsonl_files = [f for f in os.listdir(config["docs_path"]) if f.endswith('.jsonl')]
        if jsonl_files:
            print(f"  ‚úÖ {len(jsonl_files)} arquivos .jsonl encontrados:")
            for f in jsonl_files[:5]:  # Mostrar apenas os primeiros 5
                print(f"    üìù {f}")
            if len(jsonl_files) > 5:
                print(f"    ... e mais {len(jsonl_files) - 5} arquivos")
        else:
            print(f"  ‚ö†Ô∏è Nenhum arquivo .jsonl encontrado em {config['docs_path']}")
    else:
        print(f"  ‚ùå Diret√≥rio {config['docs_path']} n√£o existe")
    
    # Verificar √≠ndice
    print(f"\n{Fore.CYAN}üîç √çndice de busca:")
    if os.path.exists(config["index_path"]):
        print(f"  ‚úÖ √çndice FAISS: {config['index_path']}")
        try:
            index = faiss.read_index(config["index_path"])
            print(f"  üìä Vetores no √≠ndice: {index.ntotal}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao ler √≠ndice: {e}")
    else:
        print(f"  ‚ùå √çndice n√£o encontrado: {config['index_path']}")
    
    if os.path.exists(config["meta_path"]):
        print(f"  ‚úÖ Metadados: {config['meta_path']}")
        try:
            with open(config["meta_path"], 'r', encoding='utf-8') as f:
                lines = sum(1 for _ in f)
            print(f"  üìä Chunks de texto: {lines}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao ler metadados: {e}")
    else:
        print(f"  ‚ùå Metadados n√£o encontrados: {config['meta_path']}")
    
    # Verificar configura√ß√µes
    print(f"\n{Fore.CYAN}‚öôÔ∏è Configura√ß√µes:")
    if os.path.exists(config["settings_path"]):
        print(f"  ‚úÖ Arquivo de configura√ß√µes existe")
        try:
            with open(config["settings_path"], 'r', encoding='utf-8') as f:
                settings = json.load(f)
            print(f"  üìä Modelo de embeddings: {settings.get('embedding_model', 'N/A')}")
            print(f"  üìä Dimens√£o: {settings.get('dimension', 'N/A')}")
            print(f"  üìä Criado em: {settings.get('created_at', 'N/A')}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao ler configura√ß√µes: {e}")
    else:
        print(f"  ‚ö†Ô∏è Usando configura√ß√µes padr√£o")
    
    # Recomenda√ß√µes
    print(f"\n{Fore.YELLOW}üí° Recomenda√ß√µes:")
    if not os.path.exists(config["docs_path"]):
        print("  ‚Ä¢ Crie o diret√≥rio 'data' e adicione arquivos .jsonl")
    elif not os.path.exists(config["index_path"]):
        print("  ‚Ä¢ Execute 'Construir Base de Conhecimento' (op√ß√£o 1)")
    else:
        print("  ‚Ä¢ Sistema pronto! Use 'Iniciar Chat' (op√ß√£o 2)")
    
    wait_for_enter()

# ================================
# Constru√ß√£o de Base de Conhecimento
# ================================

def build_knowledge_base():
    """Constr√≥i a base de conhecimento de forma interativa"""
    print(f"\n{Fore.GREEN}üèóÔ∏è CONSTRUINDO BASE DE CONHECIMENTO")
    print(f"{Fore.GREEN}{'='*50}")
    
    config = DEFAULT_CONFIG.copy()
    
    # Verificar se j√° existe uma base de conhecimento
    base_exists = (os.path.exists(config["index_path"]) and 
                   os.path.exists(config["meta_path"]) and 
                   os.path.exists(config["settings_path"]))
    
    if base_exists:
        print(f"\n{Fore.YELLOW}‚ö†Ô∏è BASE DE CONHECIMENTO J√Å EXISTE!")
        print(f"{Fore.CYAN}üìä Arquivos encontrados:")
        print(f"  ‚úÖ √çndice FAISS: {config['index_path']}")
        print(f"  ‚úÖ Metadados: {config['meta_path']}")
        print(f"  ‚úÖ Configura√ß√µes: {config['settings_path']}")
        
        # Mostrar informa√ß√µes da base atual
        try:
            with open(config["settings_path"], "r", encoding="utf-8") as f:
                settings = json.load(f)
            print(f"\n{Fore.CYAN}üìã Informa√ß√µes da base atual:")
            print(f"  üìÑ Total de documentos: {settings.get('total_documents', 'N/A')}")
            print(f"  üìù Total de chunks: {settings.get('total_chunks', 'N/A')}")
            print(f"  üìÖ Criado em: {settings.get('created_at', 'N/A')}")
            print(f"  üß† Modelo: {settings.get('embedding_model', 'N/A')}")
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erro ao ler configura√ß√µes: {e}")
        
        print(f"\n{Fore.YELLOW}üîÑ OP√á√ïES DISPON√çVEIS:")
        print(f"  1Ô∏è‚É£ Reconstruir base (substituir atual)")
        print(f"  2Ô∏è‚É£ Manter base atual (cancelar)")
        
        rebuild_choice = get_user_choice(2)
        
        if rebuild_choice == 2:
            print(f"\n{Fore.GREEN}‚úÖ Opera√ß√£o cancelada. Base atual mantida.")
            wait_for_enter()
            return
        
        print(f"\n{Fore.YELLOW}üóëÔ∏è Removendo base antiga...")
        try:
            if os.path.exists(config["index_path"]):
                os.remove(config["index_path"])
                print(f"  ‚úÖ √çndice removido")
            if os.path.exists(config["meta_path"]):
                os.remove(config["meta_path"])
                print(f"  ‚úÖ Metadados removidos")
            if os.path.exists(config["settings_path"]):
                os.remove(config["settings_path"])
                print(f"  ‚úÖ Configura√ß√µes removidas")
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erro ao remover arquivos: {e}")
            wait_for_enter()
            return
        
        print(f"{Fore.GREEN}‚úÖ Base anterior removida com sucesso!")
        print(f"\n{Fore.GREEN}üèóÔ∏è RECONSTRUINDO BASE DE CONHECIMENTO...")
    
    # Verificar se diret√≥rio data existe
    if not os.path.exists(config["docs_path"]):
        print(f"{Fore.RED}‚ùå Diret√≥rio 'data' n√£o encontrado!")
        if confirm_action("Deseja criar o diret√≥rio 'data'?"):
            os.makedirs(config["docs_path"], exist_ok=True)
            print(f"{Fore.GREEN}‚úÖ Diret√≥rio 'data' criado!")
            print(f"{Fore.YELLOW}üí° Adicione seus arquivos .jsonl em '{config['docs_path']}' e tente novamente")
            wait_for_enter()
            return
        else:
            return
    
    # Ler arquivos
    documents = read_jsonl_files(config["docs_path"])
    if not documents:
        print(f"{Fore.RED}‚ùå Nenhum arquivo .jsonl encontrado ou todos est√£o vazios!")
        print(f"{Fore.YELLOW}üí° Adicione arquivos .jsonl em '{config['docs_path']}' e tente novamente")
        wait_for_enter()
        return
    
    print(f"\n{Fore.CYAN}üìä RESUMO DOS ARQUIVOS:")
    total_chars = 0
    for filepath, content in documents.items():
        filename = os.path.basename(filepath)
        char_count = len(content)
        total_chars += char_count
        print(f"  üìÑ {filename}: {char_count:,} caracteres")
    
    print(f"\n{Fore.GREEN}‚úÖ Total: {len(documents)} arquivos, {total_chars:,} caracteres")
    
    if not confirm_action("Continuar com a constru√ß√£o do √≠ndice?"):
        return
    
    try:
        # Criar chunks com estrat√©gia otimizada
        print(f"\n{Fore.BLUE}üìù Dividindo textos em chunks inteligentes...")
        chunks: list[str] = []
        metadatas: list[Metadata] = []
        
        total_content_chars = sum(len(content) for content in documents.values())
        print(f"  ÔøΩ Processando {total_content_chars:,} caracteres total")
        
        for filepath, content in documents.items():
            filename = os.path.basename(filepath)
            print(f"  üìù Processando {filename}...")
            
            # Aplica chunking inteligente
            file_chunks = chunk_text(content, config["chunk_size"], config["overlap"])
            
            # Filtra chunks muito pequenos (menos √∫teis para busca)
            filtered_chunks = [chunk for chunk in file_chunks if len(chunk.strip()) > 50]
            
            print(f"    üìä {len(content):,} chars ‚Üí {len(file_chunks)} chunks ‚Üí {len(filtered_chunks)} √∫teis")
            
            start_char = 0
            for i, chunk in enumerate(filtered_chunks):
                # Limpa e normaliza o chunk
                clean_chunk = chunk.strip()
                if clean_chunk:
                    chunks.append(clean_chunk)
                    end_char = start_char + len(chunk)
                    metadatas.append(Metadata(
                        source=filepath,
                        chunk_id=i,
                        start_char=start_char,
                        end_char=end_char
                    ))
                    start_char = end_char - config["overlap"]
        
        if not chunks:
            print(f"{Fore.RED}‚ùå Nenhum chunk v√°lido foi gerado!")
            return
        
        print(f"{Fore.GREEN}‚úÖ Criados {len(chunks)} chunks √∫teis")
        print(f"  üìè Tamanho m√©dio: {sum(len(c) for c in chunks) // len(chunks)} caracteres")
        print(f"  üìä Distribui√ß√£o de tamanhos:")
        
        # Mostra distribui√ß√£o de tamanhos para an√°lise
        sizes = [len(c) for c in chunks]
        sizes.sort()
        print(f"    M√≠nimo: {min(sizes)} chars")
        print(f"    Mediana: {sizes[len(sizes)//2]} chars")  
        print(f"    M√°ximo: {max(sizes)} chars")
        
        # Criar embeddings
        print(f"\n{Fore.BLUE}üß† Carregando modelo de embeddings...")
        print(f"  üì¶ Modelo: {config['embedding_model']}")
        model = SentenceTransformer(config["embedding_model"])
        
        print(f"{Fore.BLUE}üîÑ Gerando embeddings...")
        embeddings = []
        batch_size = 32
        
        for i in tqdm(range(0, len(chunks), batch_size), desc="Processando chunks"):
            batch = chunks[i:i+batch_size]
            batch_embeddings = model.encode(batch, show_progress_bar=False)
            embeddings.extend(batch_embeddings)
        
        embeddings_array = np.array(embeddings).astype('float32')
        print(f"{Fore.GREEN}‚úÖ Embeddings criados: {embeddings_array.shape}")
        
        # Criar √≠ndice FAISS
        print(f"\n{Fore.BLUE}üîç Construindo √≠ndice FAISS...")
        dimension = embeddings_array.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
        
        # Normalizar embeddings para cosine similarity
        faiss.normalize_L2(embeddings_array)
        index.add(embeddings_array)
        
        print(f"{Fore.GREEN}‚úÖ √çndice constru√≠do com {index.ntotal} vetores")
        
        # Criar diret√≥rios de sa√≠da
        os.makedirs(os.path.dirname(config["index_path"]), exist_ok=True)
        
        # Salvar √≠ndice
        print(f"\n{Fore.BLUE}üíæ Salvando arquivos...")
        faiss.write_index(index, config["index_path"])
        print(f"‚úÖ √çndice salvo em: {config['index_path']}")
        
        # Salvar metadados
        with open(config["meta_path"], "w", encoding="utf-8") as f:
            for i, (chunk, meta) in enumerate(zip(chunks, metadatas)):
                data = {
                    "chunk_id": i,
                    "text": chunk,
                    "source": meta.source,
                    "start_char": meta.start_char,
                    "end_char": meta.end_char
                }
                f.write(json.dumps(data, ensure_ascii=False) + "\n")
        print(f"‚úÖ Metadados salvos em: {config['meta_path']}")
        
        # Salvar configura√ß√µes
        settings = {
            "embedding_model": config["embedding_model"],
            "chunk_size": config["chunk_size"],
            "overlap": config["overlap"],
            "dimension": dimension,
            "total_chunks": len(chunks),
            "total_documents": len(documents),
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(config["settings_path"], "w", encoding="utf-8") as f:
            json.dump(settings, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Configura√ß√µes salvas em: {config['settings_path']}")
        
        print(f"\n{Fore.GREEN}üéâ BASE DE CONHECIMENTO CRIADA COM SUCESSO!")
        print(f"{Fore.GREEN}{'='*50}")
        print(f"{Fore.CYAN}üìä Estat√≠sticas:")
        print(f"  üìÑ Documentos processados: {len(documents)}")
        print(f"  üìù Chunks criados: {len(chunks)}")
        print(f"  üß† Dimens√£o dos embeddings: {dimension}")
        print(f"  üíæ Tamanho do √≠ndice: {index.ntotal} vetores")
        print(f"\n{Fore.YELLOW}üí° Agora voc√™ pode usar a op√ß√£o 2 para conversar com o chatbot!")
        
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Erro durante a constru√ß√£o da base:")
        print(f"{Fore.RED}   {str(e)}")
        print(f"\n{Fore.YELLOW}üí° Tente novamente ou consulte a ajuda (op√ß√£o 5)")
    
    wait_for_enter()

# ================================
# Chat Interativo
# ================================

def load_meta(meta_path: str) -> list[dict[str, Any]]:
    """Carrega metadados do arquivo JSONL"""
    items = []
    with open(meta_path, "r", encoding="utf-8") as f:
        for line in f:
            items.append(json.loads(line))
    return items

def search_index(query: str, index_path: str, meta_path: str, top_k: int = 3) -> list[Retrieved]:
    """Busca no √≠ndice FAISS"""
    config = DEFAULT_CONFIG
    
    # Carregar √≠ndice e metadados
    index = faiss.read_index(index_path)
    meta = load_meta(meta_path)
    
    # Criar embedding da query
    model = SentenceTransformer(config["embedding_model"])
    query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    
    # Buscar
    scores, indices = index.search(query_embedding, top_k)
    indices = indices[0]
    scores = scores[0]
    
    # Montar resultados
    results: list[Retrieved] = []
    for i, score in zip(indices, scores):
        if i == -1:
            continue
        rec = meta[i]
        metadata = Metadata(
            source=rec["source"],
            chunk_id=rec["chunk_id"],
            start_char=rec["start_char"],
            end_char=rec["end_char"]
        )
        results.append(Retrieved(text=rec["text"], meta=metadata, score=float(score)))
    
    return results

def generate_answer(contexts: list[Retrieved], question: str) -> str:
    """Gera resposta usando RAG + FLAN-T5 h√≠brido"""
    config = DEFAULT_CONFIG
    
    # Verificar qualidade dos contextos recuperados
    if not contexts:
        print(f"{Fore.YELLOW}üîç Nenhum contexto encontrado no RAG, usando FLAN-T5...")
        return flan_fallback.generate_fallback_response(question)
    
    # Calcular score m√©dio para decidir usar RAG ou FLAN-T5
    avg_score = sum(ctx.score for ctx in contexts) / len(contexts)
    min_score = min(ctx.score for ctx in contexts)
    
    # Limiar para decidir entre RAG e FLAN-T5
    RAG_THRESHOLD = 0.4
    MIN_SCORE_THRESHOLD = 0.3
    
    if avg_score < RAG_THRESHOLD or min_score < MIN_SCORE_THRESHOLD:
        print(f"{Fore.YELLOW}ü§î Contexto RAG com baixa relev√¢ncia (score: {avg_score:.2f}), usando FLAN-T5...")
        return flan_fallback.generate_fallback_response(question)
    
    # Usar RAG com contexto de alta qualidade
    print(f"{Fore.GREEN}‚úÖ Usando RAG com contexto relevante (score: {avg_score:.2f})")
    
    # Montar prompt para RAG
    context_text = "\n\n".join([
        f"[DOCUMENTO {i+1} - Relev√¢ncia: {ctx.score:.2f}]\n{ctx.text}" 
        for i, ctx in enumerate(contexts)
    ])
    
    prompt = f"""Voc√™ √© um assistente especializado da ICTA Technology. Responda APENAS com base no contexto fornecido abaixo.

IMPORTANTE: 
- Use SOMENTE as informa√ß√µes do contexto
- Se a resposta n√£o estiver clara no contexto, diga que precisa de mais informa√ß√µes
- Seja espec√≠fico e direto
- Mantenha o foco em BI, automa√ß√£o, IA e integra√ß√µes

CONTEXTO DA ICTA:
{context_text}

PERGUNTA DO CLIENTE: {question}

RESPOSTA BASEADA NO CONTEXTO:"""
    
    # Carregar modelo se necess√°rio
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = AutoTokenizer.from_pretrained(config["generation_model"])
        model = AutoModelForSeq2SeqLM.from_pretrained(config["generation_model"]).to(device)
        
        # Gerar resposta
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=10000).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=config["max_tokens"],
                do_sample=True,
                temperature=0.3,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=2
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Limpar resposta
        response = response.replace(prompt, "").strip()
        
        # Adicionar indicador de que foi resposta do RAG
        response += f"\n\nüìä *Resposta baseada na base de conhecimento da ICTA (relev√¢ncia: {avg_score:.1%})*"
        
        return response
        
    except Exception as e:
        print(f"{Fore.RED}‚ùå Erro no RAG, usando FLAN-T5 como backup: {e}")
        return flan_fallback.generate_fallback_response(question)

def hybrid_rag_query(query: str, top_k: int = 8) -> tuple[str, str]:
    """Consulta h√≠brida que combina RAG e FLAN-T5"""
    
    # 1. Tentar buscar no RAG primeiro
    try:
        search_results = search_index(query, "./index/faiss.index", "./index/meta.jsonl", top_k=top_k)
        
        if search_results:
            # Analisar qualidade dos resultados
            scores = [result.score for result in search_results]
            avg_score = sum(scores) / len(scores)
            
            print(f"{Fore.CYAN}üîç RAG encontrou {len(search_results)} resultados (score m√©dio: {avg_score:.2f})")
            
            # Gerar resposta (decide internamente entre RAG e FLAN-T5)
            answer = generate_answer(search_results, query)
            
            if avg_score >= 0.4:
                return answer, "rag"
            else:
                return answer, "flan_t5_low_rag"
        else:
            print(f"{Fore.YELLOW}ü§î RAG n√£o encontrou resultados, usando FLAN-T5...")
            answer = flan_fallback.generate_fallback_response(query)
            return answer, "flan_t5_no_rag"
            
    except Exception as e:
        print(f"{Fore.RED}‚ùå Erro no RAG: {e}")
        print(f"{Fore.YELLOW}üîÑ Usando FLAN-T5 como fallback...")
        answer = flan_fallback.generate_fallback_response(query)
        return answer, "flan_t5_error"

def print_colored(text: str, color: str = "white"):
    """Imprime texto com cor espec√≠fica"""
    colors = {
        "red": Fore.RED,
        "green": Fore.GREEN,
        "blue": Fore.BLUE,
        "yellow": Fore.YELLOW,
        "cyan": Fore.CYAN,
        "magenta": Fore.MAGENTA,
        "white": Fore.WHITE,
        "gray": Fore.LIGHTBLACK_EX
    }
    print(f"{colors.get(color, Fore.WHITE)}{text}{Style.RESET_ALL}")

def clear_screen():
    """Limpa a tela do terminal"""
    os.system('cls' if os.name == 'nt' else 'clear')

def classify_query_intent(question: str) -> dict:
    """Classifica a inten√ß√£o da pergunta usando palavras-chave simples"""
    try:
        question_lower = question.lower()
        
        # Classifica√ß√£o baseada em palavras-chave
        if any(word in question_lower for word in ['ol√°', 'oi', 'bom dia', 'boa tarde', 'boa noite', 'hello', 'hey']):
            return {"intent": "saudacao", "topic": "sauda√ß√£o", "confidence": "high"}
        
        elif any(word in question_lower for word in ['tchau', 'at√© logo', 'bye', 'adeus', 'obrigado', 'valeu']):
            return {"intent": "despedida", "topic": "despedida", "confidence": "high"}
        
        elif any(word in question_lower for word in ['totvs', 'integra√ß√£o', 'integrar', 'conectar', 'sistema', 'erp']):
            return {"intent": "integracao", "topic": "integra√ß√µes com TOTVS", "confidence": "high"}
        
        elif any(word in question_lower for word in ['bi', 'business intelligence', 'relat√≥rio', 'dashboard', 'kpi']):
            return {"intent": "bi", "topic": "Business Intelligence", "confidence": "high"}
        
        elif any(word in question_lower for word in ['automa√ß√£o', 'automatizar', 'processo', 'workflow']):
            return {"intent": "automacao", "topic": "automa√ß√£o de processos", "confidence": "high"}
        
        elif any(word in question_lower for word in ['ia', 'intelig√™ncia artificial', 'machine learning', 'ai']):
            return {"intent": "ia", "topic": "intelig√™ncia artificial", "confidence": "high"}
        
        elif any(word in question_lower for word in ['servi√ßo', 'oferecem', 'fazem', 'trabalham', 'consultoria']):
            return {"intent": "servicos", "topic": "servi√ßos da ICTA", "confidence": "high"}
        
        else:
            return {"intent": "geral", "topic": "informa√ß√µes gerais", "confidence": "medium"}
            
    except Exception as e:
        print_colored(f"‚ùå Erro na classifica√ß√£o: {e}", "red")
        return {"intent": "geral", "topic": "informa√ß√µes gerais", "confidence": "low"}

def generate_guided_response(contexts: list, question: str, intent_info: dict) -> str:
    """Gera resposta guiada com intera√ß√£o usando FLAN-T5"""
    try:
        config = DEFAULT_CONFIG
        model_name = config["generation_model"]
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
        
        # Verifica se h√° contextos relevantes
        if not contexts or all(ctx.score < 0.3 for ctx in contexts):
            # Sem contextos relevantes - resposta conversacional simples
            if intent_info.get('intent') == 'saudacao':
                return "Ol√°! üëã Sou o assistente da ICTA Technology. Como posso ajudar voc√™ hoje? Posso esclarecer d√∫vidas sobre nossos servi√ßos de BI, automa√ß√£o e IA!"
            elif intent_info.get('intent') == 'despedida':
                return "At√© logo! üëã Foi um prazer ajudar. Se precisar de mais alguma coisa sobre BI, automa√ß√£o ou IA, estarei aqui!"
            else:
                guidance_prompt = f"""Voc√™ √© um assistente da ICTA Technology. Responda de forma conversacional e √∫til.

Pergunta: {question}
T√≥pico: {intent_info.get('topic', 'geral')}

Responda de forma amig√°vel, fa√ßa perguntas se necess√°rio, e sugira como podemos ajudar.

Resposta:"""
        else:
            # Com contextos - resposta com base no conte√∫do
            context_text = "\n".join([f"- {ctx.text[:200]}..." for ctx in contexts[:2]])
            
            guidance_prompt = f"""Baseado no contexto, responda a pergunta de forma conversacional e √∫til.

Contexto: {context_text}
Pergunta: {question}

Resposta conversacional:"""
        
        inputs = tokenizer(guidance_prompt, return_tensors="pt", max_length=10000, truncation=True).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=400,
                temperature=0.5,
                do_sample=True,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        # Limpar a resposta removendo o prompt
        if "Resposta:" in response:
            response = response.split("Resposta:")[-1].strip()
        
        return response if response else "Desculpe, n√£o consegui gerar uma resposta adequada. Pode reformular sua pergunta?"
        
    except Exception as e:
        print_colored(f"‚ùå Erro na gera√ß√£o de resposta: {e}", "red")
        
        # Fallback simples baseado na inten√ß√£o
        if intent_info.get('intent') == 'saudacao':
            return "Ol√°! üëã Sou o assistente da ICTA Technology. Como posso ajudar?"
        elif intent_info.get('intent') == 'despedida':
            return "At√© logo! üëã Obrigado por usar nossos servi√ßos!"
        else:
            return f"Entendi que voc√™ est√° perguntando sobre {intent_info.get('topic', 'nossos servi√ßos')}. Pode me dar mais detalhes para te ajudar melhor?"

def suggest_related_topics(intent: str) -> list[str]:
    """Sugere t√≥picos relacionados baseado na inten√ß√£o"""
    topic_suggestions = {
        "servicos": [
            "Quais s√£o os principais servi√ßos de BI?",
            "Como funciona a automa√ß√£o de processos?",
            "Que tipos de IA voc√™s implementam?"
        ],
        "integracao": [
            "Quais m√≥dulos TOTVS voc√™s integram?",
            "Como √© feita a migra√ß√£o de dados?",
            "Qual o tempo de implementa√ß√£o?"
        ],
        "bi": [
            "Que ferramentas de BI voc√™s usam?",
            "Como criar dashboards personalizados?",
            "Quais relat√≥rios est√£o dispon√≠veis?"
        ],
        "automacao": [
            "Que processos podem ser automatizados?",
            "Qual o ROI da automa√ß√£o?",
            "Como funciona a implementa√ß√£o?"
        ],
        "ia": [
            "Que tipos de IA voc√™s desenvolvem?",
            "Como a IA pode ajudar meu neg√≥cio?",
            "Quais s√£o os casos de uso mais comuns?"
        ]
    }
    
    return topic_suggestions.get(intent, [
        "Quais s√£o os principais servi√ßos da ICTA?",
        "Como posso entrar em contato?",
        "Que tipos de projetos voc√™s fazem?"
    ])

def interactive_chat():
    """Chat interativo simples e limpo"""
    print_colored("\nüí¨ CHAT ICTA TECHNOLOGY", "cyan")
    print_colored("=" * 50, "cyan")
    
    # Verificar se h√° base de conhecimento (silenciosamente)
    rag_available = os.path.exists("./index/faiss.index")
    
    if not rag_available:
        print_colored("‚ö†Ô∏è Para melhor experi√™ncia, construa a base de conhecimento primeiro (op√ß√£o 1)", "yellow")
    
    print_colored("\nü§ñ Ol√°! Sou o assistente da ICTA Technology.", "green")
    print_colored("üí° Posso ajudar com d√∫vidas sobre BI, automa√ß√£o, IA e integra√ß√µes.", "blue")
    print_colored("\nüìù Digite sua pergunta (ou 'sair' para encerrar):", "white")
    
    conversation_history = []
    
    while True:
        print_colored("\n" + "‚îÄ" * 50, "gray")
        user_input = input(f"{Fore.YELLOW}üë§ Voc√™: {Style.RESET_ALL}").strip()
        
        if user_input.lower() in ['sair', 'exit', 'quit', 'bye']:
            print_colored("\nüëã Obrigado por usar o assistente ICTA! At√© logo!", "green")
            break
        
        if not user_input:
            print_colored("‚ùì Por favor, digite uma pergunta.", "yellow")
            continue
        
        # Salvar pergunta do usu√°rio
        conversation_history.append({"role": "user", "content": user_input, "timestamp": datetime.now().isoformat()})
        
        # Processar pergunta silenciosamente (sem mostrar detalhes t√©cnicos)
        try:
            if rag_available:
                # Tentar RAG primeiro
                search_results = search_index(user_input, "./index/faiss.index", "./index/meta.jsonl", top_k=8)
                
                if search_results:
                    # Avaliar qualidade silenciosamente
                    scores = [result.score for result in search_results]
                    avg_score = sum(scores) / len(scores)
                    
                    # Usar RAG + FLAN-T5 se qualidade √© boa
                    if avg_score >= 0.4:
                        # RAG context + FLAN-T5 processing
                        answer = generate_enhanced_answer_with_context(search_results, user_input)
                    else:
                        # Apenas FLAN-T5 sem contexto RAG
                        answer = generate_enhanced_answer_without_context(user_input)
                else:
                    # Sem resultados RAG, usar apenas FLAN-T5
                    answer = generate_enhanced_answer_without_context(user_input)
            else:
                # Apenas FLAN-T5 sem RAG
                answer = generate_enhanced_answer_without_context(user_input)
                
        except Exception:
            # Em caso de erro, usar resposta padr√£o
            answer = "Para informa√ß√µes espec√≠ficas sobre nossos servi√ßos, entre em contato com nossa equipe da ICTA Technology."
        
        # Exibir apenas a resposta, sem informa√ß√µes t√©cnicas
        print_colored(f"\nü§ñ ICTA Assistant:", "cyan")
        print_colored(answer, "white")
        
        # Salvar resposta
        conversation_history.append({
            "role": "assistant", 
            "content": answer,
            "timestamp": datetime.now().isoformat()
        })
    
    # Salvar hist√≥rico silenciosamente
    save_conversation_history(conversation_history)
    wait_for_enter()

def generate_enhanced_answer_with_context(contexts: list[Retrieved], question: str) -> str:
    """Usa modelo portugu√™s para processar RAG + pergunta"""
    try:
        if not contexts:
            return generate_enhanced_answer_without_context(question)
        
        # Extrair contexto dos melhores resultados
        context_parts = []
        for ctx in contexts[:5]:  # Usar os 5 melhores
            context_parts.append(ctx.text.strip())
        
        rag_context = "\n\n".join(context_parts)
        
        # Usar modelo portugu√™s para resposta melhor
        response = portuguese_llm.generate_enhanced_response(question, rag_context)
        
        return response
        
    except Exception:
        # Fallback para RAG direto se houver erro
        return contexts[0].text if contexts else "Entre em contato para mais informa√ß√µes sobre os servi√ßos da ICTA Technology."

def generate_enhanced_answer_without_context(question: str) -> str:
    """Usa modelo portugu√™s para perguntas gerais"""
    try:
        # Usar modelo portugu√™s diretamente
        response = portuguese_llm.generate_enhanced_response(question)
        
        # Se a resposta for muito gen√©rica, usar FLAN-T5 como fallback
        if len(response) < 20 or "Entre em contato" in response:
            # Criar respostas personalizadas para cumprimentos
            if any(word in question.lower() for word in ['ol√°', 'oi', 'bom dia', 'boa tarde', 'boa noite']):
                return "Ol√°! Sou o assistente da ICTA Technology. Posso ajudar com informa√ß√µes sobre Business Intelligence, automa√ß√£o de processos e intelig√™ncia artificial. Como posso ajud√°-lo?"
            
            if 'icta' in question.lower():
                return "A ICTA Technology √© uma consultoria especializada em Business Intelligence, automa√ß√£o de processos, intelig√™ncia artificial e integra√ß√µes com sistemas ERP. Oferecemos solu√ß√µes personalizadas para empresas."
        
        return response
        
    except Exception:
        return "Posso ajudar com informa√ß√µes sobre os servi√ßos da ICTA Technology. O que gostaria de saber?"
    
    conversation_history = []
    response_stats = {"rag": 0, "flan_t5_low_rag": 0, "flan_t5_no_rag": 0, "flan_t5_error": 0}
    
    while True:
        print_colored("\n" + "‚îÄ" * 50, "gray")
        user_input = input(f"{Fore.YELLOW}üë§ Voc√™: {Style.RESET_ALL}").strip()
        
        if user_input.lower() in ['sair', 'exit', 'quit', 'bye']:
            print_colored("\nÔøΩ ESTAT√çSTICAS DA CONVERSA:", "cyan")
            print_colored(f"   üéØ Respostas RAG: {response_stats['rag']}", "green")
            print_colored(f"   ü§ñ Respostas FLAN-T5 (baixa relev√¢ncia): {response_stats['flan_t5_low_rag']}", "yellow")
            print_colored(f"   üß† Respostas FLAN-T5 (sem contexto): {response_stats['flan_t5_no_rag']}", "blue")
            print_colored(f"   üîß Fallback por erro: {response_stats['flan_t5_error']}", "red")
            print_colored("\nÔøΩüëã Obrigado por usar o assistente h√≠brido ICTA! At√© logo!", "green")
            break
        
        if not user_input:
            print_colored("‚ùì Por favor, digite uma pergunta.", "yellow")
            continue
        
        # Salvar pergunta do usu√°rio
        conversation_history.append({"role": "user", "content": user_input, "timestamp": datetime.now().isoformat()})
        
        print_colored(f"{Fore.CYAN}üîç Processando pergunta...", "cyan")
        
        # Usar sistema h√≠brido
        if rag_available:
            try:
                answer, source_type = hybrid_rag_query(user_input, top_k=8)
                response_stats[source_type] += 1
                
                # Indicador visual do tipo de resposta
                if source_type == "rag":
                    indicator = "üéØ RAG"
                elif source_type == "flan_t5_low_rag":
                    indicator = "ü§ñ FLAN-T5 (baixa relev√¢ncia RAG)"
                elif source_type == "flan_t5_no_rag":
                    indicator = "üß† FLAN-T5 (sem contexto RAG)"
                else:
                    indicator = "üîß FLAN-T5 (fallback)"
                
                print_colored(f"   Fonte: {indicator}", "gray")
                
            except Exception as e:
                print_colored(f"‚ùå Erro no sistema h√≠brido: {e}", "red")
                answer = flan_fallback.generate_fallback_response(user_input)
                response_stats["flan_t5_error"] += 1
        else:
            # Apenas FLAN-T5 se n√£o h√° RAG
            print_colored("   üß† Usando apenas FLAN-T5 (sem RAG)", "yellow")
            answer = flan_fallback.generate_fallback_response(user_input)
            response_stats["flan_t5_no_rag"] += 1
        
        # Exibir resposta
        print_colored(f"\nü§ñ ICTA Assistant:", "cyan")
        print_colored(answer, "white")
        
        # Perguntar feedback para melhoria cont√≠nua
        print_colored(f"\nüí≠ Esta resposta foi √∫til? (s/n/parcial): ", "gray", end="")
        try:
            feedback = input().strip().lower()
            if feedback in ['n', 'n√£o', 'nao']:
                print_colored("ÔøΩ Vou tentar uma abordagem diferente na pr√≥xima!", "blue")
            elif feedback in ['parcial', 'mais ou menos']:
                print_colored("üìù Entendi, vou me esfor√ßar para ser mais preciso!", "yellow")
            elif feedback in ['s', 'sim', 'yes']:
                print_colored("üòä Fico feliz em ajudar!", "green")
        except KeyboardInterrupt:
            break
        
        # Salvar resposta com metadados
        conversation_history.append({
            "role": "assistant", 
            "content": answer,
            "source_type": source_type if rag_available else "flan_t5_only",
            "feedback": feedback if 'feedback' in locals() else None,
            "timestamp": datetime.now().isoformat()
        })
    
    # Salvar hist√≥rico
    save_conversation_history(conversation_history)
    wait_for_enter()

def save_conversation_history(history: list):
    """Salva o hist√≥rico da conversa"""
    try:
        os.makedirs("./history", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        history_file = f"./history/chat_{timestamp}.json"
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        
        print_colored(f"üíæ Conversa salva em: {history_file}", "green")
    except Exception as e:
        print_colored(f"‚ùå Erro ao salvar hist√≥rico: {e}", "red")

def start_chat():
    """Inicia o chat interativo inteligente"""
    interactive_chat()

# ================================
# Configura√ß√µes
# ================================

def show_settings():
    """Mostra e permite alterar configura√ß√µes b√°sicas"""
    print(f"\n{Fore.GREEN}‚öôÔ∏è CONFIGURA√á√ïES DO SISTEMA")
    print(f"{Fore.GREEN}{'='*40}")
    
    config = DEFAULT_CONFIG
    
    print(f"\n{Fore.CYAN}üìã Configura√ß√µes atuais:")
    print(f"  1. Modelo de embeddings: {config['embedding_model']}")
    print(f"  2. Modelo de gera√ß√£o: {config['generation_model']}")
    print(f"  3. Tamanho do chunk: {config['chunk_size']} caracteres")
    print(f"  4. Sobreposi√ß√£o: {config['overlap']} caracteres")
    print(f"  5. Documentos por busca: {config['top_k']}")
    print(f"  6. Tokens m√°ximos na resposta: {config['max_tokens']}")
    
    print(f"\n{Fore.YELLOW}üí° Dicas:")
    print(f"‚Ä¢ Chunks menores = busca mais precisa, mas pode perder contexto")
    print(f"‚Ä¢ Mais documentos por busca = respostas mais completas")
    print(f"‚Ä¢ Mais tokens = respostas mais longas")
    
    print(f"\n{Fore.LIGHTBLACK_EX}‚ÑπÔ∏è Para alterar configura√ß√µes, edite o arquivo de c√≥digo")
    print(f"‚ÑπÔ∏è Ap√≥s altera√ß√µes, reconstrua a base de conhecimento")
    
    wait_for_enter()

# ================================
# Sistema de Ajuda
# ================================

def show_help():
    """Sistema de ajuda interativo"""
    while True:
        print(f"\n{Fore.CYAN}üìö CENTRAL DE AJUDA")
        print(f"{Fore.CYAN}{'='*40}")
        
        print_menu_option(1, "‚ùì Como come√ßar", 
                         "Primeiros passos para usar o sistema")
        
        print_menu_option(2, "üìÅ Preparar documentos", 
                         "Como organizar seus arquivos .jsonl")
        
        print_menu_option(3, "üîß Solu√ß√£o de problemas", 
                         "Erros comuns e solu√ß√µes")
        
        print_menu_option(4, "üí° Dicas de uso", 
                         "Como obter melhores resultados")
        
        print_menu_option(5, "üìñ Sobre o projeto", 
                         "Informa√ß√µes t√©cnicas")
        
        print_menu_option(6, "üîô Voltar", 
                         "Retorna ao menu principal")
        
        choice = get_user_choice(6)
        
        if choice == 1:
            show_getting_started()
        elif choice == 2:
            show_document_guide()
        elif choice == 3:
            show_troubleshooting()
        elif choice == 4:
            show_usage_tips()
        elif choice == 5:
            show_about()
        elif choice == 6:
            break

def show_getting_started():
    """Guia de primeiros passos"""
    print(f"\n{Fore.GREEN}üöÄ PRIMEIROS PASSOS")
    print(f"{Fore.GREEN}{'='*30}")
    print(f"""
{Fore.CYAN}Passo 1: Preparar documentos{Style.RESET_ALL}
‚Ä¢ Crie/verifique a pasta 'data' no diret√≥rio do programa
‚Ä¢ Adicione arquivos .jsonl com suas FAQs e documentos
‚Ä¢ Use formato claro: "P: pergunta R: resposta"

{Fore.CYAN}Passo 2: Construir base de conhecimento{Style.RESET_ALL}
‚Ä¢ No menu principal, escolha op√ß√£o 1
‚Ä¢ Aguarde o processamento dos documentos
‚Ä¢ Isso criar√° um √≠ndice de busca inteligente

{Fore.CYAN}Passo 3: Conversar com o chatbot{Style.RESET_ALL}
‚Ä¢ No menu principal, escolha op√ß√£o 2
‚Ä¢ Digite suas perguntas naturalmente
‚Ä¢ O sistema buscar√° as melhores respostas

{Fore.YELLOW}üéØ Dica: Comece com poucos documentos para testar!{Style.RESET_ALL}
    """)
    wait_for_enter()

def show_document_guide():
    """Guia para prepara√ß√£o de documentos"""
    print(f"\n{Fore.GREEN}üìÅ GUIA DE DOCUMENTOS")
    print(f"{Fore.GREEN}{'='*30}")
    print(f"""
{Fore.CYAN}Estrutura recomendada:{Style.RESET_ALL}
data/
‚îú‚îÄ‚îÄ faq_geral.jsonl
‚îú‚îÄ‚îÄ produtos.jsonl
‚îú‚îÄ‚îÄ suporte.jsonl
‚îî‚îÄ‚îÄ politicas.jsonl

{Fore.CYAN}Formato dos arquivos .jsonl:{Style.RESET_ALL}
{"id": "faq-001", "source": "faq_geral", "question": "Como funciona o sistema?", "answer": "Nosso sistema utiliza intelig√™ncia artificial..."}
{"id": "faq-002", "source": "faq_geral", "question": "Quais s√£o os pre√ßos?", "answer": "Oferecemos planos a partir de R$ 99/m√™s..."}

{Fore.CYAN}Boas pr√°ticas:{Style.RESET_ALL}
‚Ä¢ Use linguagem clara e direta
‚Ä¢ Inclua palavras-chave importantes
‚Ä¢ Evite textos muito longos
‚Ä¢ Organize por temas em arquivos separados
‚Ä¢ Teste com perguntas reais dos usu√°rios

{Fore.YELLOW}üí° Exemplo de bom formato:{Style.RESET_ALL}
P: Como entrar em contato?
R: Entre em contato pelo WhatsApp (11) 99999-9999 ou 
email contato@ictatechnology.com. Atendemos de segunda 
a sexta das 9h √†s 18h.
    """)
    wait_for_enter()

def show_troubleshooting():
    """Guia de solu√ß√£o de problemas"""
    print(f"\n{Fore.GREEN}üîß SOLU√á√ÉO DE PROBLEMAS")
    print(f"{Fore.GREEN}{'='*35}")
    print(f"""
{Fore.RED}‚ùå "Nenhum arquivo .jsonl encontrado"{Style.RESET_ALL}
‚Ä¢ Verifique se a pasta 'data' existe
‚Ä¢ Confirme que h√° arquivos .jsonl na pasta
‚Ä¢ Verifique se os arquivos n√£o est√£o vazios

{Fore.RED}‚ùå "Modelo n√£o encontrado"{Style.RESET_ALL}
‚Ä¢ Primeira execu√ß√£o precisa de internet
‚Ä¢ Aguarde o download dos modelos (pode demorar)
‚Ä¢ Verifique sua conex√£o com a internet

{Fore.RED}‚ùå "Erro de mem√≥ria"{Style.RESET_ALL}
‚Ä¢ Feche outros programas pesados
‚Ä¢ Use um modelo menor (flan-t5-small)
‚Ä¢ Reduza o tamanho dos chunks no c√≥digo

{Fore.RED}‚ùå "Respostas ruins"{Style.RESET_ALL}
‚Ä¢ Melhore a qualidade dos documentos
‚Ä¢ Use textos mais espec√≠ficos
‚Ä¢ Adicione mais exemplos similares
‚Ä¢ Verifique se as palavras-chave est√£o corretas

{Fore.YELLOW}üÜò Precisa de mais ajuda?{Style.RESET_ALL}
‚Ä¢ GitHub: https://github.com/jesseff20/rag-chatbot
‚Ä¢ Issues: reporte problemas no GitHub
‚Ä¢ Email: contato@ictatechnology.com
    """)
    wait_for_enter()

def show_usage_tips():
    """Dicas de uso avan√ßado"""
    print(f"\n{Fore.GREEN}üí° DICAS DE USO AVAN√áADO")
    print(f"{Fore.GREEN}{'='*35}")
    print(f"""
{Fore.CYAN}üìù Para melhores documentos:{Style.RESET_ALL}
‚Ä¢ Use perguntas que seus clientes realmente fazem
‚Ä¢ Inclua sin√¥nimos e varia√ß√µes
‚Ä¢ Mantenha respostas focadas e diretas
‚Ä¢ Atualize regularmente o conte√∫do

{Fore.CYAN}üîç Para melhores buscas:{Style.RESET_ALL}
‚Ä¢ Use palavras-chave espec√≠ficas
‚Ä¢ Fa√ßa perguntas completas, n√£o apenas palavras
‚Ä¢ Seja espec√≠fico sobre o que quer saber
‚Ä¢ Reformule se n√£o conseguir boa resposta

{Fore.CYAN}‚öôÔ∏è Para melhor performance:{Style.RESET_ALL}
‚Ä¢ Organize documentos por tema
‚Ä¢ Mantenha arquivos com tamanho moderado
‚Ä¢ Remova informa√ß√µes duplicadas
‚Ä¢ Teste regularmente a qualidade

{Fore.CYAN}üéØ Para casos espec√≠ficos:{Style.RESET_ALL}
‚Ä¢ FAQ geral: use linguagem informal
‚Ä¢ Documenta√ß√£o t√©cnica: seja preciso
‚Ä¢ Atendimento: inclua contatos e hor√°rios
‚Ä¢ Produtos: especifica√ß√µes e pre√ßos atuais

{Fore.YELLOW}üîÑ Lembre-se: ap√≥s mudan√ßas nos documentos,{Style.RESET_ALL}
{Fore.YELLOW}reconstrua a base de conhecimento (op√ß√£o 1)!{Style.RESET_ALL}
    """)
    wait_for_enter()

def show_about():
    """Informa√ß√µes sobre o projeto"""
    print(f"\n{Fore.GREEN}üìñ SOBRE O PROJETO")
    print(f"{Fore.GREEN}{'='*30}")
    print(f"""
{Fore.CYAN}RAG Chatbot ICTA Technology{Style.RESET_ALL}
Vers√£o: 2.0 - Interface Simplificada
Data: Agosto 2025

{Fore.CYAN}üë®‚Äçüíª Desenvolvido por:{Style.RESET_ALL}
Jesse Fernandes
ICTA Technology
Email: jesse.fernandes@ictatechnology.com

{Fore.CYAN}üîß Tecnologias utilizadas:{Style.RESET_ALL}
‚Ä¢ FAISS - Busca vetorial Facebook AI
‚Ä¢ Sentence Transformers - Embeddings de texto
‚Ä¢ FLAN-T5 - Modelo de linguagem Google
‚Ä¢ Python 3.8+ - Linguagem de programa√ß√£o

{Fore.CYAN}‚ú® Caracter√≠sticas:{Style.RESET_ALL}
‚Ä¢ 100% local (sem APIs pagas)
‚Ä¢ Interface amig√°vel e interativa
‚Ä¢ Configura√ß√£o autom√°tica
‚Ä¢ Suporte a m√∫ltiplos documentos
‚Ä¢ Hist√≥rico de conversas

{Fore.CYAN}üìÑ Licen√ßa:{Style.RESET_ALL}
MIT License - Uso livre e modifica√ß√£o permitida

{Fore.CYAN}üåê Links importantes:{Style.RESET_ALL}
‚Ä¢ GitHub: https://github.com/jesseff20/rag-chatbot
‚Ä¢ Issues: https://github.com/jesseff20/rag-chatbot/issues
‚Ä¢ Documenta√ß√£o: Arquivo README.md

{Fore.YELLOW}üíù Desenvolvido com ‚ù§Ô∏è para a comunidade!{Style.RESET_ALL}
{Fore.YELLOW}Contribui√ß√µes e sugest√µes s√£o bem-vindas.{Style.RESET_ALL}
    """)
    wait_for_enter()

# ================================
# Fun√ß√£o Principal
# ================================

def main():
    """Fun√ß√£o principal com menu interativo"""
    # Configura√ß√£o inicial
    print_header()
    
    # Loop principal
    while True:
        show_main_menu()
        choice = get_user_choice(6)
        
        if choice == 1:
            build_knowledge_base()
        elif choice == 2:
            start_chat()
        elif choice == 3:
            check_system_status()
        elif choice == 4:
            show_settings()
        elif choice == 5:
            show_help()
        elif choice == 6:
            print(f"\n{Fore.GREEN}üëã Obrigado por usar o RAG Chatbot ICTA!")
            print(f"{Fore.GREEN}At√© logo!")
            break

if __name__ == "__main__":
    main()
